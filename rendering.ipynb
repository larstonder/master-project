{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a134db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!module load CUDA/11.7.0\n",
    "!PYTHONPATH=$(pwd)\"/monarch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ad8b4e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'typings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmonarch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnuplan\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NuPlan\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmonarch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplanning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moscillating_planner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OscillatingPlanner\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmonarch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m use_path\n",
      "File \u001b[0;32m~/master-project/monarch/simulator/nuplan.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnuplan\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactor_state\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mego_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EgoState\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnuplan\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactor_state\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mego_state\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CarFootprint\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtypings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SystemState, VehicleState\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtypings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Action\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msimulator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract_simulator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AbstractSimulator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'typings'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "from monarch.simulator.nuplan import NuPlan\n",
    "from monarch.planning.oscillating_planner import OscillatingPlanner\n",
    "from monarch.utils.path_utils import use_path\n",
    "from monarch.utils.image_utils import save_rgb_images_to_video, print_rgb_image\n",
    "from monarch.types.state_types import EnvState\n",
    "from monarch.rendering.omnire import OmniRe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e15f70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /cluster/home/larstond/master-project/drivestudio to sys.path\n",
      "Changed working directory to /cluster/home/larstond/master-project/drivestudio\n",
      "Working directory: /cluster/home/larstond/master-project/drivestudio\n",
      "Relative data path (relative to drivestudio): data/nuplan/processed/mini/2021.05.12.22.00.38_veh-35_01008_01518\n",
      "Absolute data path: /cluster/home/larstond/master-project/drivestudio/data/nuplan/processed/mini/2021.05.12.22.00.38_veh-35_01008_01518\n",
      "Loading data from: data/nuplan/processed/mini/2021.05.12.22.00.38_veh-35_01008_01518\n",
      "Data loaded.\n",
      "Loading checkpoint from: output/master-project/run_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:13<00:00, 22.20it/s]\n",
      "Loading dynamic masks:   2%|▏         | 7/300 [00:00<00:09, 31.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:07<00:00, 38.17it/s]\n",
      "Loading human masks:   2%|▏         | 5/300 [00:00<00:06, 47.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:07<00:00, 42.19it/s]\n",
      "Loading vehicle masks:   2%|▏         | 6/300 [00:00<00:09, 30.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:07<00:00, 40.55it/s]\n",
      "Loading sky masks:   2%|▏         | 6/300 [00:00<00:05, 50.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:05<00:00, 56.10it/s]\n",
      "Loading images:   1%|          | 3/300 [00:00<00:11, 24.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:13<00:00, 22.68it/s]\n",
      "Loading dynamic masks:   1%|▏         | 4/300 [00:00<00:09, 30.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:06<00:00, 43.02it/s]\n",
      "Loading human masks:   2%|▏         | 5/300 [00:00<00:06, 42.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:06<00:00, 42.96it/s]\n",
      "Loading vehicle masks:   1%|▏         | 4/300 [00:00<00:08, 36.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:06<00:00, 45.15it/s]\n",
      "Loading sky masks:   2%|▏         | 5/300 [00:00<00:06, 47.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:05<00:00, 55.42it/s]\n",
      "Loading images:   2%|▏         | 5/300 [00:00<00:13, 22.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:11<00:00, 25.41it/s]\n",
      "Loading dynamic masks:   3%|▎         | 8/300 [00:00<00:08, 35.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:06<00:00, 44.63it/s]\n",
      "Loading human masks:   2%|▏         | 5/300 [00:00<00:07, 40.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:06<00:00, 45.75it/s]\n",
      "Loading vehicle masks:   2%|▏         | 5/300 [00:00<00:07, 41.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:06<00:00, 49.45it/s]\n",
      "Loading sky masks:   2%|▏         | 6/300 [00:00<00:05, 57.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:05<00:00, 58.13it/s]\n",
      "Loading images:   1%|▏         | 4/300 [00:00<00:16, 18.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:12<00:00, 24.10it/s]\n",
      "Loading dynamic masks:   1%|▏         | 4/300 [00:00<00:07, 39.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:06<00:00, 48.04it/s]\n",
      "Loading human masks:   1%|▏         | 4/300 [00:00<00:07, 39.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:06<00:00, 46.77it/s]\n",
      "Loading vehicle masks:   1%|▏         | 4/300 [00:00<00:08, 32.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:06<00:00, 46.47it/s]\n",
      "Loading sky masks:   2%|▏         | 5/300 [00:00<00:07, 40.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:04<00:00, 61.34it/s]\n",
      "Loading images:   2%|▏         | 5/300 [00:00<00:11, 24.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:11<00:00, 25.22it/s]\n",
      "Loading dynamic masks:   2%|▏         | 5/300 [00:00<00:06, 45.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:06<00:00, 49.56it/s]\n",
      "Loading human masks:   1%|▏         | 4/300 [00:00<00:09, 31.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:05<00:00, 50.52it/s]\n",
      "Loading vehicle masks:   3%|▎         | 9/300 [00:00<00:06, 42.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:06<00:00, 47.90it/s]\n",
      "Loading sky masks:   2%|▏         | 6/300 [00:00<00:05, 49.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:05<00:00, 58.49it/s]\n",
      "Loading images:   2%|▏         | 5/300 [00:00<00:12, 23.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:12<00:00, 24.29it/s]\n",
      "Loading dynamic masks:   3%|▎         | 10/300 [00:00<00:05, 49.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:06<00:00, 49.93it/s]\n",
      "Loading human masks:   1%|          | 3/300 [00:00<00:12, 23.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:07<00:00, 41.95it/s]\n",
      "Loading vehicle masks:   2%|▏         | 6/300 [00:00<00:05, 54.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:05<00:00, 52.21it/s]\n",
      "Loading sky masks:   2%|▏         | 7/300 [00:00<00:04, 63.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:04<00:00, 60.15it/s]\n",
      "Loading images:   2%|▏         | 5/300 [00:00<00:12, 22.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:12<00:00, 24.53it/s]\n",
      "Loading dynamic masks:   2%|▏         | 5/300 [00:00<00:07, 40.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:06<00:00, 46.10it/s]\n",
      "Loading human masks:   1%|▏         | 4/300 [00:00<00:09, 32.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:06<00:00, 48.71it/s]\n",
      "Loading vehicle masks:   1%|▏         | 4/300 [00:00<00:07, 37.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:06<00:00, 49.16it/s]\n",
      "Loading sky masks:   2%|▏         | 5/300 [00:00<00:06, 45.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:04<00:00, 60.55it/s]\n",
      "Loading images:   1%|▏         | 4/300 [00:00<00:16, 17.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:12<00:00, 24.25it/s]\n",
      "Loading dynamic masks:   2%|▏         | 5/300 [00:00<00:07, 39.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:06<00:00, 46.59it/s]\n",
      "Loading human masks:   3%|▎         | 8/300 [00:00<00:07, 39.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:06<00:00, 49.51it/s]\n",
      "Loading vehicle masks:   2%|▏         | 6/300 [00:00<00:05, 50.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:06<00:00, 46.94it/s]\n",
      "Loading sky masks:   2%|▏         | 5/300 [00:00<00:06, 47.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:05<00:00, 57.95it/s]\n",
      "Loading SMPL: 100%|██████████| 300/300 [00:02<00:00, 142.89it/s]\n",
      "Loading lidar:   0%|          | 0/300 [00:00<?, ?it/s]/cluster/home/larstond/master-project/drivestudio/datasets/nuplan/nuplan_sourceloader.py:418: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  lidar_points = torch.from_numpy(lidar_info[:, :3]).float()\n",
      "Loading lidar: 100%|██████████| 300/300 [00:34<00:00,  8.78it/s]\n",
      "Projecting lidar pts on images for camera CAM_F0: 100%|██████████| 300/300 [01:56<00:00,  2.57it/s]\n",
      "Projecting lidar pts on images for camera CAM_L0: 100%|██████████| 300/300 [01:50<00:00,  2.72it/s]\n",
      "Projecting lidar pts on images for camera CAM_R0: 100%|██████████| 300/300 [01:49<00:00,  2.75it/s]\n",
      "Projecting lidar pts on images for camera CAM_L1: 100%|██████████| 300/300 [01:46<00:00,  2.81it/s]\n",
      "Projecting lidar pts on images for camera CAM_R1: 100%|██████████| 300/300 [01:50<00:00,  2.73it/s]\n",
      "Projecting lidar pts on images for camera CAM_L2: 100%|██████████| 300/300 [01:47<00:00,  2.80it/s]\n",
      "Projecting lidar pts on images for camera CAM_R2: 100%|██████████| 300/300 [01:50<00:00,  2.72it/s]\n",
      "Projecting lidar pts on images for camera CAM_B0: 100%|██████████| 300/300 [01:45<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing trainer models.trainers.MultiTrainer\n",
      "Initing models\n",
      "SMPLX model files found. Proceeding with the import.\n",
      "Initializing RigidNodes with {'type': 'models.nodes.RigidNodes', 'init': {'instance_max_pts': 5000, 'only_moving': True, 'traj_length_thres': 1.0}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.01, 'densify_grad_thresh': 0.0002, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.1, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 40000, 'stop_split_at': 30000, 'sh_degree': 3, 'cull_out_of_bound': True}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 'scene_radius'}, 'sh_dc': {'lr': 0.0025}, 'sh_rest': {'lr': 0.000125}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.001}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}}, 'class_name': 'RigidNodes', 'scene_scale': 200.9453887939453, 'scene_origin': tensor([178.1839,  18.1985,   9.5917], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n",
      "Initializing RigidNodes with {'type': 'models.nodes.DeformableNodes', 'init': {'instance_max_pts': 5000, 'only_moving': True, 'traj_length_thres': 0.5}, 'networks': {'D': 8, 'W': 256, 'embed_dim': 16, 'x_multires': 10, 't_multires': 10, 'deform_quat': True, 'deform_scale': False}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.1, 'densify_grad_thresh': 0.0003, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.1, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 40000, 'stop_split_at': 30000, 'sh_degree': 3, 'cull_out_of_bound': False, 'use_deformgs_for_nonrigid': True, 'use_deformgs_after': 4000, 'stop_optimizing_canonical_xyz': True}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'out_of_bound_loss': {'w': 1.0, 'stop_after': 40000}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 2.0}, 'sh_dc': {'lr': 0.0025}, 'sh_rest': {'lr': 0.000125}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.001}, 'embedding': {'lr': 0.001, 'lr_final': 0.0001}, 'deform_network': {'lr': 0.0016, 'lr_final': 0.00016, 'scale_factor': 5.0}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}}, 'class_name': 'DeformableNodes', 'scene_scale': 200.9453887939453, 'scene_origin': tensor([178.1839,  18.1985,   9.5917], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n",
      "Initializing RigidNodes with {'type': 'models.nodes.SMPLNodes', 'init': {'only_moving': True, 'traj_length_thres': 0.5}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}, 'knn_reg': {'lambda_std_q': 0.001, 'lambda_std_s': 0.001, 'lambda_std_o': 0.001, 'lambda_std_shs_dc': 0.001, 'lambda_std_shs_rest': 0.001}, 'max_s_square_reg': {'w': 0.05}, 'x_offset': {'w': 0.2}, 'voxel_deformer_reg': {'lambda_std_w': 0.6, 'lambda_std_w_rest': 0.5, 'lambda_w_norm': 0.6, 'lambda_w_rest_norm': 0.3}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 3.0}, 'sh_dc': {'lr': 0.005}, 'sh_rest': {'lr': 0.00025}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.005}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}, 'smpl_rotation': {'lr': 5e-05, 'lr_final': 1e-05}, 'w_dc_vox': {'lr': 0.0001, 'lr_final': 1e-05, 'opt_after': 10000}}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.01, 'densify_grad_thresh': 0.0003, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.5, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 5000, 'stop_split_at': 20000, 'sh_degree': 1, 'opacity_init_value': 0.99, 'ball_gaussians': False, 'constrain_xyz_offset': False, 'knn_update_interval': 100, 'knn_neighbors': 3, 'use_voxel_deformer': True, 'freeze_x': False, 'freeze_o': False, 'freeze_q': False, 'freeze_s': False, 'freeze_shs_dc': False, 'freeze_shs_rest': False}, 'class_name': 'SMPLNodes', 'scene_scale': 200.9453887939453, 'scene_origin': tensor([178.1839,  18.1985,   9.5917], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from output/master-project/run_final/checkpoint_final.pth\n",
      "Loading Background from checkpoint\n",
      "Loading RigidNodes from checkpoint\n",
      "Loading DeformableNodes from checkpoint\n",
      "Loading SMPLNodes from checkpoint\n",
      "Using predefined pose: da_pose\n",
      "Loading Sky from checkpoint\n",
      "Loading Affine from checkpoint\n",
      "Loading CamPose from checkpoint\n",
      "Resuming from checkpoint: output/master-project/run_final/checkpoint_final.pth, starting at step 40000\n",
      "Successfully initialized OmniRe environment model\n",
      "Restored original sys.path\n",
      "Restored working directory to /cluster/home/larstond/master-project\n"
     ]
    }
   ],
   "source": [
    "renderer = None\n",
    "\n",
    "# Now use the context manager for clean path handling\n",
    "with use_path(\"drivestudio\", True):\n",
    "    # Define paths relative to the drivestudio directory\n",
    "    relative_data_path = \"data/nuplan/processed/mini/2021.05.12.22.00.38_veh-35_01008_01518\"\n",
    "    # relative_data_path = \"data/nuplan/processed/mini/2021.05.12.22.28.35_veh-35_00620_01164\"\n",
    "    relative_checkpoint_path = (\n",
    "        \"output/master-project/run_final\"\n",
    "    )\n",
    "\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "    print(f\"Relative data path (relative to drivestudio): {relative_data_path}\")\n",
    "    print(f\"Absolute data path: {os.path.abspath(relative_data_path)}\")\n",
    "\n",
    "    # Check if these files exist in this context\n",
    "    if not os.path.exists(relative_data_path):\n",
    "        print(\n",
    "            f\"ERROR: Data path not found at {os.path.abspath(relative_data_path)}\"\n",
    "        )\n",
    "    if not os.path.exists(relative_checkpoint_path):\n",
    "        print(\n",
    "            f\"ERROR: Checkpoint directory not found at {os.path.abspath(relative_checkpoint_path)}\"\n",
    "        )\n",
    "\n",
    "    # Only initialize if files exist\n",
    "    if os.path.exists(relative_data_path) and os.path.exists(\n",
    "        relative_checkpoint_path\n",
    "    ):\n",
    "        renderer = OmniRe(relative_data_path, relative_checkpoint_path)\n",
    "        print(\"Successfully initialized OmniRe environment model\")\n",
    "    else:\n",
    "        print(\"Failed to initialize environment model due to missing files\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06035998",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Python script to render images using the OmniRe model.\n",
    "This script is designed to work with the DriveStudio framework and\n",
    "is tailored for the NuPlan dataset.\n",
    "It handles the rendering of images based on the simulation state\n",
    "and the camera parameters.\n",
    "\"\"\"\n",
    "\n",
    "import traceback\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import copy\n",
    "from typing import Dict\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from monarch.types.state_types import SystemState, EnvState, VehicleState\n",
    "from monarch.rendering.omnire import OmniReSetup\n",
    "from drivestudio.datasets.base.pixel_source import get_rays\n",
    "from drivestudio.models.trainers.scene_graph import MultiTrainer\n",
    "\n",
    "CORRECTION_MATRIX = np.array([[-0.90322894,  0.42915905,  0.        ],\n",
    "                              [-0.42915905, -0.90322894,  0.        ],\n",
    "                              [ 0.,          0.,          1.        ]])\n",
    "\n",
    "class OmniReBackup:\n",
    "    \"\"\"\n",
    "    OmniReModel class for rendering images using the OmniRe model.\n",
    "    This class is designed to work with the DriveStudio framework and\n",
    "    is tailored for the NuPlan dataset.\n",
    "    It handles the rendering of images based on the simulation state\n",
    "    and the camera parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, setup: OmniReSetup, original_state: SystemState):\n",
    "        with use_path(\"drivestudio\", True):\n",
    "            setup._initialize_trainer()\n",
    "        self.cfg = setup.cfg\n",
    "        self.device = setup.device\n",
    "        self.trainer: MultiTrainer = setup.trainer\n",
    "        print(f\"Trainer type: {type(self.trainer)}\")\n",
    "        self.dataset = setup.dataset\n",
    "        self._initialize()\n",
    "\n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize the dataset and frame data\"\"\"\n",
    "        render_traj = self.dataset.get_novel_render_traj()\n",
    "        self.frame_data = self.dataset.pixel_source.prepare_novel_view_render_data(\n",
    "            self.dataset.pixel_source, render_traj[\"front_center_interp\"]\n",
    "        )[0]\n",
    "\n",
    "    def _get_delta_pos(self, last_pos: VehicleState, current_pos: VehicleState) -> VehicleState:\n",
    "        \"\"\"Get the delta position between two positions\"\"\"\n",
    "        delta_x = float(current_pos.x) - float(last_pos.x)\n",
    "        delta_y = float(current_pos.y) - float(last_pos.y)\n",
    "        delta_z = float(current_pos.z) - float(last_pos.z)\n",
    "        delta_heading = float(current_pos.heading) - float(last_pos.heading)\n",
    "        return VehicleState(x=delta_x, y=delta_y, z=delta_z, heading=delta_heading, id=current_pos.id)\n",
    "    \n",
    "    def _edit_nodes(self, vehicle_deltas: list[VehicleState] = None, apply_correction: bool = True):\n",
    "        \"\"\"Edit the nodes in the scene graph based on vehicle position deltas.\n",
    "        \n",
    "        Args:\n",
    "            vehicle_deltas: List of VehicleState objects representing the change in position and heading for each vehicle.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if vehicle_deltas is None or len(vehicle_deltas) == 0:\n",
    "                print(\"No vehicle deltas provided, skipping node editing\")\n",
    "                return\n",
    "                \n",
    "            # print(f\"=== Applying changes to {len(vehicle_deltas)} vehicles ===\")\n",
    "            \n",
    "            if \"RigidNodes\" in self.trainer.gaussian_classes.keys():\n",
    "                rigid_model = self.trainer.models[\"RigidNodes\"]\n",
    "                \n",
    "                with torch.no_grad():  # Disable gradient tracking during editing\n",
    "                    for vehicle_state in vehicle_deltas:\n",
    "                        instance_id = vehicle_state.id\n",
    "                        \n",
    "                        if instance_id is None:\n",
    "                            print(\"Error: Vehicle delta id is None. Skipping edit for this vehicle.\")\n",
    "                            continue\n",
    "                        \n",
    "                        if hasattr(rigid_model, 'track_token_to_model_id_map') and instance_id in rigid_model.track_token_to_model_id_map:\n",
    "                            model_id = rigid_model.track_token_to_model_id_map[instance_id]\n",
    "                        else:\n",
    "                            try:\n",
    "                                dataset_id = int(instance_id)\n",
    "                                if hasattr(rigid_model, 'dataset_id_to_model_id_map') and dataset_id in rigid_model.dataset_id_to_model_id_map:\n",
    "                                    model_id = rigid_model.dataset_id_to_model_id_map[dataset_id]\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                                                \n",
    "                        delta_vector = np.array([vehicle_state.x, vehicle_state.y, vehicle_state.z])\n",
    "                        \n",
    "                        if apply_correction:\n",
    "                            correction_matrix = torch.tensor(CORRECTION_MATRIX, device=rigid_model.device)\n",
    "                            delta_vector_tensor = torch.tensor(delta_vector, device=rigid_model.device)\n",
    "                            updated_vector = (correction_matrix @ delta_vector_tensor).float()\n",
    "                            \n",
    "                            delta_vector = updated_vector\n",
    "                        \n",
    "                        delta_x = delta_vector[0]\n",
    "                        delta_y = delta_vector[1]\n",
    "                        delta_z = delta_vector[2]\n",
    "                        \n",
    "                        delta_translation = torch.tensor([delta_y, delta_x, delta_z], device=rigid_model.device).float()\n",
    "                        delta_rotation_quat = torch.tensor([1.0, 0.0, 0.0, vehicle_state.heading], device=rigid_model.device)  # Convert heading to quaternion as needed\n",
    "                        \n",
    "                        rigid_model.translate_instance(model_id, delta_translation)\n",
    "                        rigid_model.rotate_instance(model_id, delta_rotation_quat)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in edit_nodes: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "    def get_sensor_input(self, original_state: SystemState, last_state: SystemState, current_state: SystemState, apply_correction: bool = True) -> EnvState:\n",
    "        frame_data = copy.deepcopy(self.frame_data)\n",
    "        self.trainer.set_eval()\n",
    "        \n",
    "        # ---- UPDATE OTHER VEHICLES ----\n",
    "        vehicle_deltas = []\n",
    "        ego_delta = self._get_delta_pos(original_state.ego_pos, current_state.ego_pos)\n",
    "\n",
    "        if hasattr(current_state, 'vehicle_pos_list') and hasattr(last_state, 'vehicle_pos_list'):\n",
    "            for i in range(len(current_state.vehicle_pos_list)):\n",
    "                \n",
    "                current_id_to_check = current_state.vehicle_pos_list[i].id\n",
    "                for j in range(len(last_state.vehicle_pos_list)):\n",
    "                    if last_state.vehicle_pos_list[j].id == current_id_to_check:\n",
    "                        delta_pos = self._get_delta_pos(last_state.vehicle_pos_list[j], current_state.vehicle_pos_list[i])                    \n",
    "                        vehicle_deltas.append(delta_pos)\n",
    "                        break\n",
    "    \n",
    "        \n",
    "        self._edit_nodes(vehicle_deltas, apply_correction)\n",
    "        \n",
    "        # ---- UPDATE CAMERA POSITION ----\n",
    "        c2w = frame_data[\"cam_infos\"][\"camera_to_world\"].to(self.device)\n",
    "        \n",
    "        delta_x, delta_y, delta_z = ego_delta.x, ego_delta.y, ego_delta.z\n",
    "        \n",
    "        if apply_correction:\n",
    "            delta_vector = np.array([ego_delta.x, ego_delta.y, ego_delta.z])\n",
    "            correction_matrix = CORRECTION_MATRIX.astype(delta_vector.dtype)\n",
    "            updated_vector = correction_matrix @ delta_vector\n",
    "            \n",
    "            delta_x = updated_vector[0]\n",
    "            delta_y = updated_vector[1]\n",
    "            delta_z = updated_vector[2]\n",
    "        \n",
    "        delta_translation = torch.tensor([delta_y, delta_x, delta_z], device=self.device)\n",
    "        \n",
    "        c2w[:3, 3] = c2w[:3, 3] + delta_translation\n",
    "        \n",
    "        current_rotation = c2w[:3, :3].cpu().numpy()\n",
    "        \n",
    "        current_rot_obj = R.from_matrix(current_rotation)\n",
    "        delta_heading_rot = R.from_euler('y', ego_delta.heading, degrees=False)\n",
    "        \n",
    "        new_rotation = current_rot_obj * delta_heading_rot\n",
    "        c2w[:3, :3] = torch.tensor(new_rotation.as_matrix(), device=self.device)\n",
    "\n",
    "        frame_data[\"cam_infos\"][\"camera_to_world\"] = c2w\n",
    "\n",
    "        cam_id = 0\n",
    "\n",
    "        H, W = (\n",
    "            self.dataset.pixel_source.camera_data[cam_id].HEIGHT,\n",
    "            self.dataset.pixel_source.camera_data[cam_id].WIDTH,\n",
    "        )\n",
    "\n",
    "        x, y = torch.meshgrid(\n",
    "            torch.arange(H),\n",
    "            torch.arange(W),\n",
    "            indexing=\"xy\",\n",
    "        )\n",
    "        x, y = x.flatten(), y.flatten()\n",
    "\n",
    "        intrinsics = frame_data[\"cam_infos\"][\"intrinsics\"]\n",
    "\n",
    "        x, y, c2w, intrinsics = (\n",
    "            x.to(self.device),\n",
    "            y.to(self.device),\n",
    "            c2w.to(self.device),\n",
    "            intrinsics.to(self.device),\n",
    "        )\n",
    "\n",
    "        origins, viewdirs, direction_norm = get_rays(x, y, c2w, intrinsics)\n",
    "        origins = origins.reshape(H, W, 3)\n",
    "        viewdirs = viewdirs.reshape(H, W, 3)\n",
    "        direction_norm = direction_norm.reshape(H, W, 1)\n",
    "\n",
    "        frame_data[\"image_infos\"][\"origins\"] = origins\n",
    "        frame_data[\"image_infos\"][\"viewdirs\"] = viewdirs\n",
    "        frame_data[\"image_infos\"][\"direction_norm\"] = direction_norm\n",
    "        frame_data[\"cam_infos\"][\"height\"] = torch.tensor(H, device=self.device)\n",
    "        frame_data[\"cam_infos\"][\"width\"] = torch.tensor(W, device=self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Move data to GPU\n",
    "            for key, value in frame_data[\"cam_infos\"].items():\n",
    "                frame_data[\"cam_infos\"][key] = value.cuda(non_blocking=True)\n",
    "            for key, value in frame_data[\"image_infos\"].items():\n",
    "                frame_data[\"image_infos\"][key] = value.cuda(non_blocking=True)\n",
    "\n",
    "            # Perform rendering\n",
    "            outputs = self.trainer(\n",
    "                image_infos=frame_data[\"image_infos\"],\n",
    "                camera_infos=frame_data[\"cam_infos\"],\n",
    "                novel_view=True,\n",
    "            )\n",
    "\n",
    "            # Extract RGB image and mask\n",
    "            rgb = outputs[\"rgb\"].cpu().numpy().clip(min=1.0e-6, max=1 - 1.0e-6)\n",
    "\n",
    "            return EnvState(rgb_image=rgb, depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7bea52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from monarch.simulator.abstract_simulator import AbstractSimulator\n",
    "from monarch.rendering.abstract_renderer import AbstractRenderer\n",
    "from monarch.planning.abstract_planner import AbstractPlanner\n",
    "\n",
    "def planner_test(simulator: AbstractSimulator, env: AbstractRenderer, planner: AbstractPlanner, n_steps: int) -> list[EnvState]:\n",
    "    sensor_outputs: list[Env_state] = []\n",
    "    original_state = simulator.get_state()\n",
    "    \n",
    "    current_state = original_state\n",
    "    \n",
    "    for i in tqdm(range(n_steps)):\n",
    "        if not simulator.simulation.is_simulation_running():\n",
    "            break\n",
    "        \n",
    "        # apply_correction = i == 2\n",
    "        \n",
    "        # print(f\"CALCULATING STEP {i + 1}\")\n",
    "        last_state = current_state\n",
    "        current_state = simulator.get_state()\n",
    "        \n",
    "        # x, y, heading = 0, 0, 0\n",
    "        # x_last, y_last, heading_last = 0, 0, 0\n",
    "        \n",
    "        # # get the vehicle with track id b582d95cb714544d\n",
    "        # for vehicle in current_state.vehicle_pos_list:\n",
    "        #     if vehicle.id == \"b582d95cb714544d\":\n",
    "        #         # print(f\"track id: {vehicle.id}\")\n",
    "        #         x = vehicle.x\n",
    "        #         y = vehicle.y\n",
    "        #         heading = vehicle.heading\n",
    "        \n",
    "        # for vehicle in last_state.vehicle_pos_list:\n",
    "        #     if vehicle.id == \"b582d95cb714544d\":\n",
    "        #         # print(f\"track id: {vehicle.id}\")\n",
    "        #         x_last = vehicle.x\n",
    "        #         y_last = vehicle.y\n",
    "        #         heading_last = vehicle.heading\n",
    "        \n",
    "        # print(f\"x: {x}\")\n",
    "        # print(f\"y: {y}\")\n",
    "        # print(f\"heading: {heading}\")\n",
    "    \n",
    "        # print(f\"x delta: {x - x_last}\")\n",
    "        # print(f\"y delta: {y - y_last}\")\n",
    "        # print(f\"heading delta: {heading - heading_last}\")\n",
    "                \n",
    "        # Environment\n",
    "        sensor_output: Env_state = env.get_sensor_input(original_state, last_state, current_state, True)\n",
    "        \n",
    "        sensor_outputs.append(sensor_output)\n",
    "\n",
    "        # Planner\n",
    "        current_input = simulator.get_planner_input()\n",
    "        trajectory = planner.compute_planner_trajectory(current_input)\n",
    "        # print(trajectory)\n",
    "\n",
    "        # Simulator\n",
    "        simulator.do_action(trajectory)\n",
    "\n",
    "    with open(\"output.txt\", \"w\") as f:\n",
    "        f.write(\"TESTING PLANNER CONFIGURATION\\n\")\n",
    "        f.write(f\"Planner: {planner.name}\\n\")\n",
    "        f.write(f\"Type: {planner.observation_type}\\n\")\n",
    "\n",
    "    return sensor_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a151d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import Type\n",
    "from nuplan.common.actor_state.ego_state import EgoState\n",
    "from nuplan.common.actor_state.state_representation import TimePoint\n",
    "from nuplan.common.actor_state.vehicle_parameters import (\n",
    "    get_pacifica_parameters,\n",
    "    VehicleParameters,\n",
    ")\n",
    "from nuplan.planning.simulation.trajectory.abstract_trajectory import AbstractTrajectory\n",
    "from nuplan.planning.simulation.trajectory.interpolated_trajectory import (\n",
    "    InterpolatedTrajectory,\n",
    ")\n",
    "from nuplan.planning.simulation.planner.abstract_planner import (\n",
    "    AbstractPlanner,\n",
    "    PlannerInitialization,\n",
    "    PlannerInput,\n",
    ")\n",
    "from nuplan.planning.simulation.observation.observation_type import (\n",
    "    DetectionsTracks,\n",
    "    Sensors,\n",
    "    Observation,\n",
    ")  # Use Sensors for images and DetectionTracks for agents that does not use its observations\n",
    "from nuplan.planning.simulation.controller.motion_model.kinematic_bicycle import (\n",
    "    KinematicBicycleModel,\n",
    ")\n",
    "\n",
    "NUPLAN_MAP_VERSION = os.getenv(\"NUPLAN_MAP_VERSION\", \"nuplan-maps-v1.0\")\n",
    "\n",
    "\n",
    "class OscillatingPlanner:\n",
    "    \"\"\"\n",
    "    Oscillates from starside to portside.\n",
    "    Once max steering_angle is reached, the vehicle starts turning in the opposite direction creating a sine-wave trajectory.\n",
    "    The purpose of the planner is to test whether the simulator handles turning effectively\n",
    "    The Planner does not take into account the observations it receives\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        horizon_seconds: int,\n",
    "        sampling_time: float,\n",
    "        max_steering_angle: float,\n",
    "        steering_angle_increment,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor for OscillatingPlanner\n",
    "        :param horizon_seconds: [s] time horizon being run.\n",
    "        :param sampling_time: [s] sampling timestep.\n",
    "        :param max_velocity: [m/s] ego max velocity.\n",
    "        :param max_steering_angle: [rad] max_ego_steering angle.\n",
    "        NOTE:\n",
    "        - Might update acceleration to apply steering even with constant velocity (accelerates in a given direction)\n",
    "        - Might be able to remove self.vehicle param\n",
    "        -- Did not find another motion model so the one in simple_planner is used.\n",
    "        \"\"\"\n",
    "        # Init variables from params\n",
    "        self.horizon_seconds = TimePoint(int(horizon_seconds * 1e6))\n",
    "        self.sampling_time = TimePoint(int(sampling_time * 1e6))\n",
    "        self.max_steering_angle = max_steering_angle\n",
    "        self.steering_angle_increment = steering_angle_increment\n",
    "\n",
    "        # Init variables not part of params\n",
    "        # self.steering_angle = 1.0  # Added steering angle. Don't know if it used yet\n",
    "        self.max_velocity = 20.0  # Just in case the car accelerates out of control\n",
    "        self.acceleration = np.array([2.0, 0.0])  # Default acceleration [x, y]\n",
    "        self.vehicle = get_pacifica_parameters()\n",
    "        self.motion_model = KinematicBicycleModel(self.vehicle)\n",
    "\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Inherited, see superclass.\"\"\"\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def observation_type(self) -> Type[Observation]:\n",
    "        \"\"\"Inherited, see superclass\"\"\"\n",
    "        return DetectionsTracks\n",
    "\n",
    "    def compute_planner_trajectory(\n",
    "        self, current_input: PlannerInput\n",
    "    ) -> AbstractTrajectory:\n",
    "        static_velocity_magnitude = 2.0\n",
    "\n",
    "        history = current_input.history\n",
    "        ego_state = history.ego_states[-1]\n",
    "        previous_ego_state = history.ego_states[\n",
    "            -2 if len(history.ego_states) > 1 else -1\n",
    "        ]\n",
    "\n",
    "        trajectory: list[EgoState] = [ego_state]\n",
    "        current_steering_angle = ego_state.tire_steering_angle\n",
    "        previous_steering_angle = previous_ego_state.tire_steering_angle\n",
    "        for i in range(int(self.horizon_seconds.time_us / self.sampling_time.time_us)):\n",
    "            new_steering_angle = self._get_new_steering_angle(\n",
    "                current_steering_angle, previous_steering_angle\n",
    "            )\n",
    "\n",
    "            split_state = ego_state.to_split_state()\n",
    "            split_state.linear_states[-1] = new_steering_angle\n",
    "            split_state.linear_states[3] = static_velocity_magnitude * math.cos(\n",
    "                new_steering_angle\n",
    "            )\n",
    "            split_state.linear_states[4] = static_velocity_magnitude * math.sin(\n",
    "                new_steering_angle\n",
    "            )\n",
    "            split_state.linear_states[5] = 0\n",
    "            split_state.linear_states[6] = 0\n",
    "\n",
    "            state = EgoState.from_split_state(split_state)\n",
    "            state = self.motion_model.propagate_state(\n",
    "                state, state.dynamic_car_state, self.sampling_time\n",
    "            )\n",
    "            trajectory.append(state)\n",
    "\n",
    "            previous_steering_angle = current_steering_angle\n",
    "            current_steering_angle = new_steering_angle\n",
    "\n",
    "        return InterpolatedTrajectory(trajectory)\n",
    "\n",
    "    def _get_new_steering_angle(\n",
    "        self, current_steering_angle: float, previous_steering_angle: float\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Helper function for computing new steering angle\n",
    "        :param current_steering_angle: [rad] angle of ego vehicle at current state.\n",
    "        :param previous_steering_angle: [rad] angle of ego vehicle at previous state.\n",
    "        :return angle: [rad] Incremented angle\n",
    "        \"\"\"\n",
    "        angle = current_steering_angle\n",
    "        prev_angle = previous_steering_angle\n",
    "        # Handle edge cases:\n",
    "        if angle >= self.max_steering_angle:\n",
    "            angle = angle - self.steering_angle_increment\n",
    "        elif abs(angle) >= self.max_steering_angle:\n",
    "            angle = angle + self.steering_angle_increment\n",
    "\n",
    "        # Handle default case\n",
    "        else:\n",
    "            angle = (\n",
    "                angle + self.steering_angle_increment\n",
    "                if angle > prev_angle\n",
    "                else angle - self.steering_angle_increment\n",
    "            )\n",
    "\n",
    "        return angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24739d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script initializes a NuPlan simulator and\n",
    "provides methods to get the current state of the simulation\n",
    "and perform actions based on a given trajectory.\n",
    "It uses the NuPlan database and maps to create a simulation environment.\n",
    "It also includes a function to create a waypoint from a given point.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from omegaconf import OmegaConf\n",
    "from nuplan.common.actor_state.oriented_box import OrientedBox\n",
    "from nuplan.common.actor_state.vehicle_parameters import get_pacifica_parameters\n",
    "from nuplan.common.actor_state.state_representation import StateSE2\n",
    "from nuplan.common.actor_state.waypoint import Waypoint\n",
    "from nuplan.planning.scenario_builder.nuplan_db.nuplan_scenario_builder import (\n",
    "    NuPlanScenarioBuilder,\n",
    ")\n",
    "from nuplan.planning.scenario_builder.scenario_filter import ScenarioFilter\n",
    "from nuplan.planning.simulation.trajectory.abstract_trajectory import AbstractTrajectory\n",
    "from nuplan.planning.simulation.trajectory.interpolated_trajectory import (\n",
    "    InterpolatedTrajectory,\n",
    ")\n",
    "from nuplan.planning.simulation.observation.tracks_observation import TracksObservation\n",
    "from nuplan.planning.simulation.history.simulation_history_buffer import (\n",
    "    SimulationHistoryBuffer,\n",
    ")\n",
    "from nuplan.planning.simulation.simulation_time_controller.step_simulation_time_controller import (\n",
    "    StepSimulationTimeController,\n",
    ")\n",
    "from nuplan.planning.simulation.simulation import Simulation\n",
    "from nuplan.planning.simulation.simulation_setup import SimulationSetup\n",
    "from nuplan.planning.script.builders.worker_pool_builder import build_worker\n",
    "from nuplan.planning.simulation.controller.perfect_tracking import (\n",
    "    PerfectTrackingController,\n",
    ")\n",
    "from nuplan.planning.simulation.trajectory.interpolated_trajectory import (\n",
    "    InterpolatedTrajectory,\n",
    ")\n",
    "from nuplan.common.actor_state.dynamic_car_state import DynamicCarState\n",
    "from nuplan.common.actor_state.state_representation import StateVector2D\n",
    "from nuplan.common.actor_state.ego_state import EgoState\n",
    "from nuplan.common.actor_state.ego_state import CarFootprint\n",
    "from monarch.types.state_types import SystemState, VehicleState\n",
    "from monarch.types.action import Action\n",
    "from monarch.simulator.abstract_simulator import AbstractSimulator\n",
    "\n",
    "\n",
    "NUPLAN_DATA_ROOT = os.getenv(\"NUPLAN_DATA_ROOT\", \"/data/sets/nuplan\")\n",
    "NUPLAN_MAPS_ROOT = os.getenv(\"NUPLAN_MAPS_ROOT\", \"/data/sets/nuplan/maps\")\n",
    "NUPLAN_DB_FILES = os.getenv(\n",
    "    \"NUPLAN_DB_FILES\", \"/data/sets/nuplan/nuplan-v1.1/splits/mini\"\n",
    ")\n",
    "NUPLAN_MAP_VERSION = os.getenv(\"NUPLAN_MAP_VERSION\", \"nuplan-maps-v1.0\")\n",
    "NUPLAN_SENSOR_ROOT = f\"{NUPLAN_DATA_ROOT}/nuplan-v1.1/sensor_blobs\"\n",
    "DB_FILE = f\"{NUPLAN_DATA_ROOT}/nuplan-v1.1/splits/mini/2021.05.12.22.28.35_veh-35_00620_01164.db\"\n",
    "MAP_NAME = \"us-nv-las-vegas\"\n",
    "\n",
    "\n",
    "class NuPlanCopy():\n",
    "    \"\"\"\n",
    "    NuPlan simulator class that initializes the NuPlan simulation environment.\n",
    "    It uses the NuPlan database and maps to create a simulation environment.\n",
    "    It provides methods to get the current state of the simulation and perform\n",
    "    actions based on a given trajectory.\n",
    "    NOTE: https://github.com/motional/nuplan-devkit/blob/master/docs/metrics_description.md metrics description\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scenario_name: str, start_time: float):\n",
    "        super().__init__()\n",
    "        print(\"Initializing NuPlan simulator...\")\n",
    "        \n",
    "        db_file = f\"{NUPLAN_DATA_ROOT}/nuplan-v1.1/splits/mini/{scenario_name}.db\"\n",
    "        \n",
    "        scenario_builder = NuPlanScenarioBuilder(\n",
    "            data_root=NUPLAN_DATA_ROOT,\n",
    "            map_root=NUPLAN_MAPS_ROOT,\n",
    "            sensor_root=NUPLAN_SENSOR_ROOT,\n",
    "            db_files=[db_file],\n",
    "            map_version=NUPLAN_MAP_VERSION,\n",
    "            vehicle_parameters=get_pacifica_parameters(),\n",
    "            include_cameras=False,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        scenario_filter = ScenarioFilter(\n",
    "            log_names=[scenario_name],\n",
    "            scenario_types=None,\n",
    "            scenario_tokens=None,\n",
    "            map_names=None,\n",
    "            num_scenarios_per_type=None,\n",
    "            limit_total_scenarios=None,\n",
    "            timestamp_threshold_s=start_time,\n",
    "            ego_displacement_minimum_m=None,\n",
    "            expand_scenarios=False,\n",
    "            remove_invalid_goals=False,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        worker_config = OmegaConf.create(\n",
    "            {\n",
    "                \"worker\": {\n",
    "                    \"_target_\": \"nuplan.planning.utils.multithreading.worker_sequential.Sequential\",\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        worker = build_worker(worker_config)\n",
    "        self.scenario = scenario_builder.get_scenarios(scenario_filter, worker)[0]\n",
    "\n",
    "        time_controller = StepSimulationTimeController(self.scenario)        \n",
    "        observation = TracksObservation(self.scenario)\n",
    "        controller = PerfectTrackingController(self.scenario)\n",
    "\n",
    "        simulation_setup = SimulationSetup(\n",
    "            time_controller=time_controller,\n",
    "            observations=observation,\n",
    "            ego_controller=controller,\n",
    "            scenario=self.scenario,\n",
    "        )\n",
    "\n",
    "        self.simulation = Simulation(\n",
    "            simulation_setup=simulation_setup,\n",
    "            callback=None,\n",
    "            simulation_history_buffer_duration=2.0,\n",
    "        )\n",
    "        self.simulation.reset()\n",
    "\n",
    "        self.simulation.initialize()\n",
    "\n",
    "        planner_input = self.simulation.get_planner_input()\n",
    "        history: SimulationHistoryBuffer = planner_input.history\n",
    "        self.original_ego_state, self.original_observation_state = history.current_state\n",
    "        self.ego_vehicle_oriented_box = self.original_ego_state.waypoint.oriented_box\n",
    "\n",
    "        print(\"NuPlan init completed\")\n",
    "\n",
    "    def get_planner_input(self):\n",
    "        return self.simulation.get_planner_input()\n",
    "\n",
    "    def get_state(self) -> SystemState:\n",
    "        planner_input = self.simulation.get_planner_input()\n",
    "        history = planner_input.history\n",
    "        ego_state, observation_state = history.current_state\n",
    "\n",
    "        ego_pos: VehicleState = VehicleState(\n",
    "            x=ego_state.waypoint.center.x,\n",
    "            y=ego_state.waypoint.center.y,\n",
    "            # x=0,\n",
    "            # y=0,\n",
    "            z=606.740,  # height of camera\n",
    "            heading=ego_state.waypoint.heading,\n",
    "            id=-1,\n",
    "        )\n",
    "        agent_pos_list: list[VehicleState] = [\n",
    "            VehicleState(\n",
    "                x=agent.center.x, y=agent.center.y, z=606.740, heading=agent.center.heading, id=agent.track_token\n",
    "            )\n",
    "            for agent in observation_state.tracked_objects.get_agents()\n",
    "        ]\n",
    "        state = SystemState(\n",
    "            ego_pos=ego_pos,\n",
    "            vehicle_pos_list=agent_pos_list,\n",
    "            timestamp=ego_state.waypoint.time_point,\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def do_action(self, trajectory: AbstractTrajectory):\n",
    "        self.simulation.propagate(trajectory)\n",
    "\n",
    "    def sandbox_eval(self):\n",
    "        \"\"\"\n",
    "        This function is for testing evaluation of states in the NuPlan simulator.\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c290b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NuPlan simulator...\n",
      "NuPlan init completed\n",
      "Added /cluster/home/larstond/master-project/drivestudio to sys.path\n",
      "Changed working directory to /cluster/home/larstond/master-project/drivestudio\n",
      "Initing models\n",
      "Initializing RigidNodes with {'type': 'models.nodes.RigidNodes', 'init': {'instance_max_pts': 5000, 'only_moving': True, 'traj_length_thres': 1.0}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.01, 'densify_grad_thresh': 0.0002, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.1, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 40000, 'stop_split_at': 30000, 'sh_degree': 3, 'cull_out_of_bound': True}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 'scene_radius'}, 'sh_dc': {'lr': 0.0025}, 'sh_rest': {'lr': 0.000125}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.001}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}}, 'class_name': 'RigidNodes', 'scene_scale': 200.9166259765625, 'scene_origin': tensor([178.1445,  18.1460,   9.5911], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n",
      "Initializing RigidNodes with {'type': 'models.nodes.DeformableNodes', 'init': {'instance_max_pts': 5000, 'only_moving': True, 'traj_length_thres': 0.5}, 'networks': {'D': 8, 'W': 256, 'embed_dim': 16, 'x_multires': 10, 't_multires': 10, 'deform_quat': True, 'deform_scale': False}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.1, 'densify_grad_thresh': 0.0003, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.1, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 40000, 'stop_split_at': 30000, 'sh_degree': 3, 'cull_out_of_bound': False, 'use_deformgs_for_nonrigid': True, 'use_deformgs_after': 4000, 'stop_optimizing_canonical_xyz': True}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'out_of_bound_loss': {'w': 1.0, 'stop_after': 40000}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 2.0}, 'sh_dc': {'lr': 0.0025}, 'sh_rest': {'lr': 0.000125}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.001}, 'embedding': {'lr': 0.001, 'lr_final': 0.0001}, 'deform_network': {'lr': 0.0016, 'lr_final': 0.00016, 'scale_factor': 5.0}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}}, 'class_name': 'DeformableNodes', 'scene_scale': 200.9166259765625, 'scene_origin': tensor([178.1445,  18.1460,   9.5911], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n",
      "Initializing RigidNodes with {'type': 'models.nodes.SMPLNodes', 'init': {'only_moving': True, 'traj_length_thres': 0.5}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}, 'knn_reg': {'lambda_std_q': 0.001, 'lambda_std_s': 0.001, 'lambda_std_o': 0.001, 'lambda_std_shs_dc': 0.001, 'lambda_std_shs_rest': 0.001}, 'max_s_square_reg': {'w': 0.05}, 'x_offset': {'w': 0.2}, 'voxel_deformer_reg': {'lambda_std_w': 0.6, 'lambda_std_w_rest': 0.5, 'lambda_w_norm': 0.6, 'lambda_w_rest_norm': 0.3}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 3.0}, 'sh_dc': {'lr': 0.005}, 'sh_rest': {'lr': 0.00025}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.005}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}, 'smpl_rotation': {'lr': 5e-05, 'lr_final': 1e-05}, 'w_dc_vox': {'lr': 0.0001, 'lr_final': 1e-05, 'opt_after': 10000}}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.01, 'densify_grad_thresh': 0.0003, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.5, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 5000, 'stop_split_at': 20000, 'sh_degree': 1, 'opacity_init_value': 0.99, 'ball_gaussians': False, 'constrain_xyz_offset': False, 'knn_update_interval': 100, 'knn_neighbors': 3, 'use_voxel_deformer': True, 'freeze_x': False, 'freeze_o': False, 'freeze_q': False, 'freeze_s': False, 'freeze_shs_dc': False, 'freeze_shs_rest': False}, 'class_name': 'SMPLNodes', 'scene_scale': 200.9166259765625, 'scene_origin': tensor([178.1445,  18.1460,   9.5911], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from output/master-project/run_final/checkpoint_final.pth\n",
      "Loading Sky from checkpoint\n",
      "Loading Affine from checkpoint\n",
      "Loading CamPose from checkpoint\n",
      "Loading Background from checkpoint\n",
      "Loading RigidNodes from checkpoint\n",
      "Loading DeformableNodes from checkpoint\n",
      "Loading SMPLNodes from checkpoint\n",
      "Using predefined pose: da_pose\n",
      "Resuming from checkpoint: output/master-project/run_final/checkpoint_final.pth, starting at step 40000\n",
      "Restored original sys.path\n",
      "Restored working directory to /cluster/home/larstond/master-project\n",
      "Trainer type: <class 'models.trainers.scene_graph.MultiTrainer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [01:25<00:00,  3.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video with 299 frames\n",
      "Video settings: brightness=1.0, gamma=1.2\n",
      "Frame 0: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (640, 360) to (640, 368) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 10: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 20: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 30: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 40: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 50: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 60: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 70: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 80: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 90: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 100: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 110: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 120: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 130: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 140: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 150: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 160: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 170: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 180: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 190: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 200: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 210: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 220: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 230: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 240: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 250: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 260: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 270: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 280: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Frame 290: shape=(360, 640, 3), dtype=float32, min=0.0000, max=1.0000\n",
      "Video saved to output_video.mp4 with 299 frames at 20 fps\n"
     ]
    }
   ],
   "source": [
    "# from package_name.planning.oscillating_planner import OscillatingPlanner\n",
    "from monarch.utils.image_utils import save_rgb_images_to_video\n",
    "from nuplan.planning.simulation.planner.simple_planner import SimplePlanner\n",
    "from monarch.rendering.omnire import OmniRe\n",
    "\n",
    "render_keys = [\"rgbs\"]\n",
    "\n",
    "# simulator0 = NuPlan(\"2021.05.12.22.00.38_veh-35_01008_01518\")\n",
    "simulator = NuPlanCopy(\"2021.05.12.22.00.38_veh-35_01008_01518\", 1000)\n",
    "\n",
    "# print(simulator0.get_state().timestamp)\n",
    "# print(simulator.get_state().timestamp)\n",
    "\n",
    "# env = OmniReBackup(env_setup, simulator.get_state())\n",
    "planner = OscillatingPlanner(horizon_seconds=1.0, sampling_time=1.0, max_steering_angle=math.pi/8, steering_angle_increment=0.5)\n",
    "simple_planner = SimplePlanner(horizon_seconds=1.0, sampling_time=1.0, acceleration=np.array([5.0, 0.0]), max_velocity=10.0, steering_angle=0.0)\n",
    "# planner = OscillatingPlanner(horizon_seconds=1.0, sampling_time=1.0, max_steering_angle=math.pi/8, steering_angle_increment=0.01)\n",
    "n_steps = 299\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the simulation.\"\"\"\n",
    "    # planner_test(simulator, env, planner, n_steps)\n",
    "    results = planner_test(simulator, env, simple_planner, n_steps)\n",
    "    # print_rgb_image(results[].rgb_image, gamma=2.2)\n",
    "    \n",
    "    save_rgb_images_to_video(results, \"output_video.mp4\", brightness=1.0, gamma=1.2)\n",
    "    # save_rgb_images_to_video(results, \"output_video.mp4\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94746c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NuPlan simulator...\n",
      "NuPlan init completed\n",
      "Added /cluster/home/larstond/master-project/drivestudio to sys.path\n",
      "Changed working directory to /cluster/home/larstond/master-project/drivestudio\n",
      "Initing models\n",
      "Initializing RigidNodes with {'type': 'models.nodes.RigidNodes', 'init': {'instance_max_pts': 5000, 'only_moving': True, 'traj_length_thres': 1.0}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.01, 'densify_grad_thresh': 0.0002, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.1, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 40000, 'stop_split_at': 30000, 'sh_degree': 3, 'cull_out_of_bound': True}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 'scene_radius'}, 'sh_dc': {'lr': 0.0025}, 'sh_rest': {'lr': 0.000125}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.001}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}}, 'class_name': 'RigidNodes', 'scene_scale': 200.94561767578125, 'scene_origin': tensor([178.1981,  18.2255,   9.5902], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n",
      "Initializing RigidNodes with {'type': 'models.nodes.DeformableNodes', 'init': {'instance_max_pts': 5000, 'only_moving': True, 'traj_length_thres': 0.5}, 'networks': {'D': 8, 'W': 256, 'embed_dim': 16, 'x_multires': 10, 't_multires': 10, 'deform_quat': True, 'deform_scale': False}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.1, 'densify_grad_thresh': 0.0003, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.1, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 40000, 'stop_split_at': 30000, 'sh_degree': 3, 'cull_out_of_bound': False, 'use_deformgs_for_nonrigid': True, 'use_deformgs_after': 4000, 'stop_optimizing_canonical_xyz': True}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'out_of_bound_loss': {'w': 1.0, 'stop_after': 40000}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 2.0}, 'sh_dc': {'lr': 0.0025}, 'sh_rest': {'lr': 0.000125}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.001}, 'embedding': {'lr': 0.001, 'lr_final': 0.0001}, 'deform_network': {'lr': 0.0016, 'lr_final': 0.00016, 'scale_factor': 5.0}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}}, 'class_name': 'DeformableNodes', 'scene_scale': 200.94561767578125, 'scene_origin': tensor([178.1981,  18.2255,   9.5902], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n",
      "Initializing RigidNodes with {'type': 'models.nodes.SMPLNodes', 'init': {'only_moving': True, 'traj_length_thres': 0.5}, 'reg': {'sharp_shape_reg': {'w': 1.0, 'step_interval': 10, 'max_gauss_ratio': 10.0}, 'temporal_smooth_reg': {'trans': {'w': 0.01, 'smooth_range': 5}}, 'knn_reg': {'lambda_std_q': 0.001, 'lambda_std_s': 0.001, 'lambda_std_o': 0.001, 'lambda_std_shs_dc': 0.001, 'lambda_std_shs_rest': 0.001}, 'max_s_square_reg': {'w': 0.05}, 'x_offset': {'w': 0.2}, 'voxel_deformer_reg': {'lambda_std_w': 0.6, 'lambda_std_w_rest': 0.5, 'lambda_w_norm': 0.6, 'lambda_w_rest_norm': 0.3}}, 'optim': {'xyz': {'lr': 0.00016, 'lr_final': 1.6e-06, 'scale_factor': 3.0}, 'sh_dc': {'lr': 0.005}, 'sh_rest': {'lr': 0.00025}, 'opacity': {'lr': 0.05}, 'scaling': {'lr': 0.005}, 'rotation': {'lr': 0.005}, 'ins_rotation': {'lr': 1e-05, 'lr_final': 5e-06}, 'ins_translation': {'lr': 0.0005, 'lr_final': 0.0001}, 'smpl_rotation': {'lr': 5e-05, 'lr_final': 1e-05}, 'w_dc_vox': {'lr': 0.0001, 'lr_final': 1e-05, 'opt_after': 10000}}, 'ctrl': {'warmup_steps': 500, 'reset_alpha_interval': 4000, 'refine_interval': 100, 'sh_degree_interval': 1000, 'n_split_samples': 2, 'reset_alpha_value': 0.01, 'densify_grad_thresh': 0.0003, 'densify_size_thresh': 0.002, 'cull_alpha_thresh': 0.005, 'cull_scale_thresh': 0.5, 'cull_screen_size': 0.15, 'split_screen_size': 0.05, 'stop_screen_size_at': 5000, 'stop_split_at': 20000, 'sh_degree': 1, 'opacity_init_value': 0.99, 'ball_gaussians': False, 'constrain_xyz_offset': False, 'knn_update_interval': 100, 'knn_neighbors': 3, 'use_voxel_deformer': True, 'freeze_x': False, 'freeze_o': False, 'freeze_q': False, 'freeze_s': False, 'freeze_shs_dc': False, 'freeze_shs_rest': False}, 'class_name': 'SMPLNodes', 'scene_scale': 200.94561767578125, 'scene_origin': tensor([178.1981,  18.2255,   9.5902], device='cuda:0'), 'num_train_images': 2400, 'device': device(type='cuda')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from output/master-project/run_final/checkpoint_final.pth\n",
      "Loading Sky from checkpoint\n",
      "Loading Affine from checkpoint\n",
      "Loading CamPose from checkpoint\n",
      "Loading Background from checkpoint\n",
      "Loading RigidNodes from checkpoint\n",
      "Loading DeformableNodes from checkpoint\n",
      "Loading SMPLNodes from checkpoint\n",
      "Using predefined pose: da_pose\n",
      "Resuming from checkpoint: output/master-project/run_final/checkpoint_final.pth, starting at step 40000\n",
      "Restored original sys.path\n",
      "Restored working directory to /cluster/home/larstond/master-project\n",
      "Trainer type: <class 'models.trainers.scene_graph.MultiTrainer'>\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from monarch.utils.image_utils import save_rgb_image\n",
    "\n",
    "def experiment_1():\n",
    "    \"\"\"\n",
    "    Experiment 1:\n",
    "    - \n",
    "    \"\"\"\n",
    "    \n",
    "    simulator = NuPlanCopy(\"2021.05.12.22.00.38_veh-35_01008_01518\")\n",
    "    env = OmniRe(env_setup, simulator.get_state())\n",
    "    planner = OscillatingPlanner(horizon_seconds=1.0, sampling_time=1.0, max_steering_angle=math.pi/8, steering_angle_increment=0.2)\n",
    "    \n",
    "    n_steps = 299\n",
    "    sensor_outputs = planner_test(simulator, env, planner, n_steps)\n",
    "    \n",
    "    for i in enumerate(sensor_outputs):\n",
    "        if (i % 100 == 0):\n",
    "            print(f\"Step {i}\")\n",
    "            print(f\"Ego state: {sensor_outputs[i].ego_state}\")\n",
    "            print(f\"Vehicle state: {sensor_outputs[i].vehicle_state}\")\n",
    "            print(f\"Timestamp: {sensor_outputs[i].timestamp}\")\n",
    "            print(f\"RGB image: {sensor_outputs[i].rgb_image}\")\n",
    "            print(f\"Depth image: {sensor_outputs[i].depth_image}\")\n",
    "        save_rgb_image(sensor_outputs[i].rgb_image, f\"experiments/experiment_1/rgb_{i}.png\")\n",
    "            \n",
    "\n",
    "def experiment_2():\n",
    "    \"\"\"\n",
    "    Experiment 2:\n",
    "    - \n",
    "    \"\"\"\n",
    "    simulator = NuPlanCopy(\"2021.05.12.22.00.38_veh-35_01008_01518\", 1000)\n",
    "    env = OmniRe(env_setup, simulator.get_state())\n",
    "    original_state = simulator.get_state()\n",
    "    n_steps = 1\n",
    "    sensor_outputs = []\n",
    "\n",
    "    # Start with the initial state\n",
    "    last_state = copy.deepcopy(original_state)\n",
    "    current_state = copy.deepcopy(original_state)\n",
    "    \n",
    "    sensor_outputs.append(env.get_sensor_input(original_state, last_state, current_state))\n",
    "\n",
    "    for j in range(n_steps):\n",
    "        # Move the vehicle in the current_state\n",
    "        for i, vehicle in enumerate(current_state.vehicle_pos_list):\n",
    "            if vehicle.id == \"b582d95cb714544d\":\n",
    "                print(f\"Before: Vehicle {vehicle.id} at y={vehicle.y}\")\n",
    "                current_state.vehicle_pos_list[i].y = vehicle.y - n_steps * 1\n",
    "                print(f\"After:  Vehicle {vehicle.id} at y={current_state.vehicle_pos_list[i].y}\")\n",
    "\n",
    "        # Pass the modified states to the environment\n",
    "        sensor_output = env.get_sensor_input(original_state, last_state, current_state)\n",
    "        sensor_outputs.append(sensor_output)\n",
    "\n",
    "        # Prepare for next step\n",
    "        last_state = copy.deepcopy(original_state)\n",
    "\n",
    "    for j, sensor_output in enumerate(sensor_outputs):\n",
    "        save_rgb_image(sensor_output, f\"experiments/experiment_3/exp_y_{j}.png\", brightness=1.0, gamma=1.2)\n",
    "    \n",
    "\n",
    "def experiment_3():\n",
    "    \"\"\"\n",
    "    Experiment 2:\n",
    "    - \n",
    "    \"\"\"\n",
    "    simulator = NuPlanCopy(\"2021.05.12.22.00.38_veh-35_01008_01518\", 1000)\n",
    "    env = OmniRe(env_setup, simulator.get_state())\n",
    "    original_state = simulator.get_state()\n",
    "    sensor_outputs = []\n",
    "\n",
    "    # Start with the initial state\n",
    "    last_state = copy.deepcopy(original_state)\n",
    "    current_state = copy.deepcopy(original_state)\n",
    "    \n",
    "    # sensor_outputs.append(env.get_sensor_input(original_state, last_state, current_state))\n",
    "\n",
    "    for i in range(1, 5):\n",
    "        # Move the ego vehicle in the current_state\n",
    "        current_state.ego_pos.x = current_state.ego_pos.x + i * 0.5\n",
    "\n",
    "        # Pass the modified states to the environment\n",
    "        sensor_output = env.get_sensor_input(original_state, last_state, current_state)\n",
    "        sensor_outputs.append(sensor_output)\n",
    "\n",
    "        # Prepare for next step\n",
    "        last_state = copy.deepcopy(original_state)\n",
    "\n",
    "    for i, sensor_output in enumerate(sensor_outputs):\n",
    "        save_rgb_image(sensor_output, f\"experiments/experiment_3/exp_pos_{i}.png\", brightness=1.0, gamma=1.2)\n",
    "\n",
    "experiment_3()\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec769a5",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Implement env update function in environment:\n",
    "    - Update speed and angle of ego_vehicle based on output from nn\n",
    "    - Update position of car based on temporal difference and average velocity\n",
    "\n",
    "- Create RL environment for loss computing (custom loss function)\n",
    "    - Loss function can only be activated using the simulator (NuPlan) and not the environment (OmniRe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
