{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfff27a0",
   "metadata": {},
   "source": [
    "# Main simulator loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12c13c",
   "metadata": {},
   "source": [
    "## Imports, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d534ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from path_utils import use_path\n",
    "\n",
    "base_dir = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "# Use relative paths from the base directory\n",
    "config_path = str(base_dir / \"configs/datasets/nuplan/8cams_undistorted.yaml\")\n",
    "checkpoint_path = str(base_dir / \"output/master-project/run_omnire_undistorted_8cams_0\")\n",
    "n_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d48ea",
   "metadata": {},
   "source": [
    "## Initialize simulator, environment model and the random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eef672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NuPlan simulator...\n",
      "<nuplan.common.actor_state.ego_state.EgoState object at 0x7f92f8c87d90>\n",
      "NuPlan initialized.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script initializes a NuPlan simulator and\n",
    "provides methods to get the current state of the simulation\n",
    "and perform actions based on a given trajectory.\n",
    "It uses the NuPlan database and maps to create a simulation environment.\n",
    "It also includes a function to create a waypoint from a given point.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from state_types import State, Position\n",
    "from omegaconf import OmegaConf\n",
    "from nuplan.common.actor_state.oriented_box import OrientedBox\n",
    "from nuplan.common.actor_state.vehicle_parameters import get_pacifica_parameters\n",
    "from nuplan.common.actor_state.state_representation import StateSE2\n",
    "from nuplan.common.actor_state.waypoint import Waypoint\n",
    "from nuplan.planning.scenario_builder.nuplan_db.nuplan_scenario_builder import NuPlanScenarioBuilder\n",
    "from nuplan.planning.scenario_builder.scenario_filter import ScenarioFilter\n",
    "from nuplan.planning.simulation.observation.tracks_observation import TracksObservation\n",
    "from nuplan.planning.simulation.simulation_time_controller.step_simulation_time_controller import (\n",
    "    StepSimulationTimeController,\n",
    ")\n",
    "from nuplan.planning.simulation.simulation import Simulation\n",
    "from nuplan.planning.simulation.simulation_setup import SimulationSetup\n",
    "from nuplan.planning.script.builders.worker_pool_builder import build_worker\n",
    "from nuplan.planning.simulation.controller.perfect_tracking import PerfectTrackingController\n",
    "from nuplan.planning.simulation.trajectory.interpolated_trajectory import InterpolatedTrajectory\n",
    "from nuplan.common.actor_state.dynamic_car_state import DynamicCarState\n",
    "from nuplan.common.actor_state.state_representation import StateVector2D\n",
    "from nuplan.common.actor_state.ego_state import EgoState\n",
    "from nuplan.common.actor_state.ego_state import CarFootprint\n",
    "\n",
    "\n",
    "NUPLAN_DATA_ROOT = os.getenv('NUPLAN_DATA_ROOT', '/data/sets/nuplan')\n",
    "NUPLAN_MAPS_ROOT = os.getenv('NUPLAN_MAPS_ROOT', '/data/sets/nuplan/maps')\n",
    "NUPLAN_DB_FILES = os.getenv('NUPLAN_DB_FILES', '/data/sets/nuplan/nuplan-v1.1/splits/mini')\n",
    "NUPLAN_MAP_VERSION = os.getenv('NUPLAN_MAP_VERSION', 'nuplan-maps-v1.0')\n",
    "NUPLAN_SENSOR_ROOT = f\"{NUPLAN_DATA_ROOT}/nuplan-v1.1/sensor_blobs\"\n",
    "DB_FILE = f\"{NUPLAN_DATA_ROOT}/nuplan-v1.1/splits/mini/2021.05.12.22.28.35_veh-35_00620_01164.db\"\n",
    "MAP_NAME = \"us-nv-las-vegas\"\n",
    "\n",
    "class Simulator:\n",
    "    \"\"\"Base class for the simulator.\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Get the current state of the simulation.\"\"\"\n",
    "        raise NotImplementedError(\"This method should be overridden in subclasses.\")\n",
    "\n",
    "    def do_action(self, action):\n",
    "        \"\"\"Perform an action in the simulation.\"\"\"\n",
    "        raise NotImplementedError(\"This method should be overridden in subclasses.\")\n",
    "\n",
    "class NuPlan(Simulator):\n",
    "    \"\"\"\n",
    "    NuPlan simulator class that initializes the NuPlan simulation environment.\n",
    "    It uses the NuPlan database and maps to create a simulation environment.\n",
    "    It provides methods to get the current state of the simulation and perform\n",
    "    actions based on a given trajectory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Initializing NuPlan simulator...\")\n",
    "        scenario_builder = NuPlanScenarioBuilder(\n",
    "            data_root=NUPLAN_DATA_ROOT,\n",
    "            map_root=NUPLAN_MAPS_ROOT,\n",
    "            sensor_root=NUPLAN_SENSOR_ROOT,\n",
    "            db_files=[DB_FILE],\n",
    "            map_version=NUPLAN_MAP_VERSION,\n",
    "            vehicle_parameters=get_pacifica_parameters(),\n",
    "            include_cameras=False,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        scenario_filter = ScenarioFilter(\n",
    "            log_names = [\"2021.05.12.22.28.35_veh-35_00620_01164\"],\n",
    "            scenario_types = None,\n",
    "            scenario_tokens = None,\n",
    "            map_names = None,\n",
    "            num_scenarios_per_type = None,\n",
    "            limit_total_scenarios = None,\n",
    "            timestamp_threshold_s = None,\n",
    "            ego_displacement_minimum_m = None,\n",
    "            expand_scenarios = False,\n",
    "            remove_invalid_goals = False,\n",
    "            shuffle = False\n",
    "        )\n",
    "\n",
    "        worker_config = OmegaConf.create({\n",
    "            'worker': {\n",
    "                '_target_': 'nuplan.planning.utils.multithreading.worker_sequential.Sequential',\n",
    "            }\n",
    "        })\n",
    "\n",
    "        worker = build_worker(worker_config)\n",
    "        scenario = scenario_builder.get_scenarios(scenario_filter, worker)[0]\n",
    "\n",
    "        time_controller = StepSimulationTimeController(scenario)\n",
    "        observation = TracksObservation(scenario)\n",
    "        controller = PerfectTrackingController(scenario)\n",
    "\n",
    "        simulation_setup = SimulationSetup(\n",
    "            time_controller=time_controller,\n",
    "            observations=observation,\n",
    "            ego_controller=controller,\n",
    "            scenario=scenario\n",
    "        )\n",
    "\n",
    "        self.simulation = Simulation(\n",
    "            simulation_setup=simulation_setup,\n",
    "            callback=None,\n",
    "            simulation_history_buffer_duration=2.0\n",
    "        )\n",
    "\n",
    "        self.simulation.initialize()\n",
    "\n",
    "\n",
    "        planner_input = self.simulation.get_planner_input()\n",
    "        history = planner_input.history\n",
    "        self.original_ego_state, self.original_observation_state = history.current_state\n",
    "        \n",
    "        print(self.original_ego_state)\n",
    "\n",
    "        self.ego_vehicle_oriented_box = self.original_ego_state.waypoint.oriented_box\n",
    "\n",
    "        print(\"NuPlan initialized.\")\n",
    "\n",
    "    def get_state(self) -> State:\n",
    "        planner_input = self.simulation.get_planner_input()\n",
    "        history = planner_input.history\n",
    "        ego_state, observation_state = history.current_state\n",
    "\n",
    "        ego_pos: Position = Position(\n",
    "            x=ego_state.waypoint.center.x,\n",
    "            y=ego_state.waypoint.center.y,\n",
    "            # x=0,\n",
    "            # y=0,\n",
    "            z=606.740,  # height of camera\n",
    "            heading=ego_state.waypoint.heading\n",
    "        )\n",
    "        agent_pos_list: list[Position] = [\n",
    "            Position(\n",
    "                x=agent.center.x,\n",
    "                y=agent.center.y,\n",
    "                z=2,\n",
    "                heading=agent.center.heading\n",
    "            )\n",
    "            for agent in observation_state.tracked_objects.get_agents()\n",
    "        ]\n",
    "        state = State(\n",
    "            ego_pos=ego_pos,\n",
    "            vehicle_pos_list=agent_pos_list,\n",
    "            timestamp=ego_state.waypoint.time_point\n",
    "        )\n",
    "        return state\n",
    "\n",
    "    def do_action(self, action):\n",
    "        trajectory = action\n",
    "        interpolated_trajectory = self.create_interpolated_trajectory(trajectory)\n",
    "        self.simulation.propagate(interpolated_trajectory)\n",
    "\n",
    "    def create_interpolated_trajectory(self, trajectory):\n",
    "        \"\"\"\n",
    "        Create an interpolated trajectory from a given trajectory.\n",
    "        :param trajectory: The trajectory to create the interpolated trajectory from.\n",
    "        :return: The created interpolated trajectory.\n",
    "        \"\"\"\n",
    "        # Convert Waypoints to EgoStates\n",
    "        ego_states = []\n",
    "        vehicle_parameters = get_pacifica_parameters()\n",
    "        \n",
    "        for waypoint in trajectory:\n",
    "            # Extract data from the waypoint\n",
    "            time_point = waypoint.time_point\n",
    "            oriented_box = waypoint.oriented_box\n",
    "            velocity = waypoint.velocity\n",
    "            \n",
    "            # Create a dynamic car state with speed from velocity\n",
    "            speed = (velocity.x**2 + velocity.y**2)**0.5  # Calculate speed from velocity components\n",
    "            \n",
    "            car_footprint = CarFootprint(\n",
    "                center=oriented_box.center,\n",
    "                vehicle_parameters=vehicle_parameters,\n",
    "            )\n",
    "            \n",
    "            dynamic_car_state = DynamicCarState(\n",
    "                rear_axle_to_center_dist=vehicle_parameters.cog_position_from_rear_axle,\n",
    "                rear_axle_velocity_2d=StateVector2D(velocity.x, velocity.y),\n",
    "                rear_axle_acceleration_2d=StateVector2D(0, 0)  # Assuming no acceleration for simplicity\n",
    "            )\n",
    "            \n",
    "            # Create an EgoState\n",
    "            ego_state = EgoState(\n",
    "                time_point=time_point,\n",
    "                car_footprint=car_footprint,\n",
    "                dynamic_car_state=dynamic_car_state,\n",
    "                tire_steering_angle=0.0,  # Assuming no steering angle for simplicity\n",
    "                is_in_auto_mode=True,\n",
    "            )\n",
    "            \n",
    "            ego_states.append(ego_state)\n",
    "        \n",
    "        # Create an interpolated trajectory with EgoState objects\n",
    "        return InterpolatedTrajectory(ego_states)\n",
    "\n",
    "    def create_waypoint_from_point(self, point):\n",
    "        \"\"\"\n",
    "        Create a waypoint from a point.\n",
    "        :param point: The point to create the waypoint from.\n",
    "        :return: The created waypoint.\n",
    "        \"\"\"\n",
    "        pose = StateSE2(point.x, point.y, point.yaw)\n",
    "        oriented_box = OrientedBox(\n",
    "            pose,\n",
    "            width=self.ego_vehicle_oriented_box.width,\n",
    "            length=self.ego_vehicle_oriented_box.length,\n",
    "            height=self.ego_vehicle_oriented_box.height\n",
    "        )\n",
    "        return Waypoint(\n",
    "            time_point=point.time_point,\n",
    "            oriented_box=oriented_box,\n",
    "            velocity=point.velocity\n",
    "        )\n",
    "\n",
    "simulator = NuPlan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673af314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered local import context\n",
      "Adding ../drivestudio to sys.path\n",
      "Current sys.path: ['/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/ray/thirdparty_files', '/cluster/home/larstond/.conda/envs/master/lib/python39.zip', '/cluster/home/larstond/.conda/envs/master/lib/python3.9', '/cluster/home/larstond/.conda/envs/master/lib/python3.9/lib-dynload', '', '/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages', '/cluster/home/larstond/master-project/drivestudio/third_party/smplx', '/cluster/home/larstond/master-project/nuplan-devkit']\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from state_types import State\n",
    "from setup import OmniReSetup\n",
    "\n",
    "OPENCV2DATASET = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [-1, 0, 0, 0],\n",
    "    [0, -1, 0, 0],\n",
    "    [0, 0, 0, 1]\n",
    "], dtype=np.float32)\n",
    "\n",
    "class OmniReModel:\n",
    "    def __init__(self, setup: OmniReSetup):\n",
    "        self.data_cfg = setup.data_cfg\n",
    "        self.train_cfg = setup.train_cfg\n",
    "        self.trainer = setup.trainer\n",
    "        self.dataset = setup.dataset\n",
    "        self.device = setup.device\n",
    "        self.camera_matrix_cache = {}\n",
    "    \n",
    "    def render_single_frame(self, frame_data: dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Render a single frame based on provided frame data.\n",
    "        \n",
    "        Args:\n",
    "            frame_data (dict): Dictionary containing camera and image info for the frame\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: The rendered RGB image as a numpy array\n",
    "        \"\"\"\n",
    "        self.trainer.set_eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Create copies of the dictionaries to avoid modifying originals\n",
    "            cam_infos = {}\n",
    "            image_infos = {}\n",
    "            \n",
    "            # Move camera info tensors to GPU\n",
    "            for key, value in frame_data[\"cam_infos\"].items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    cam_infos[key] = value.to(self.device)\n",
    "                else:\n",
    "                    cam_infos[key] = value\n",
    "            \n",
    "            # Move image info tensors to GPU\n",
    "            for key, value in frame_data[\"image_infos\"].items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    image_infos[key] = value.to(self.device)\n",
    "                else:\n",
    "                    image_infos[key] = value\n",
    "\n",
    "            # Perform rendering - explicitly set novel_view=True as in DriveStudio\n",
    "            outputs = self.trainer(\n",
    "                image_infos=image_infos,\n",
    "                camera_infos=cam_infos,\n",
    "                novel_view=True\n",
    "            )\n",
    "\n",
    "            # Clip RGB output to valid range exactly as DriveStudio does\n",
    "            if \"rgb\" in outputs:\n",
    "                outputs[\"rgb\"] = outputs[\"rgb\"].clamp(min=1.e-6, max=1-1.e-6)\n",
    "\n",
    "            # Extract RGB image and convert to numpy\n",
    "            rgb = outputs[\"rgb\"].cpu().numpy()\n",
    "            \n",
    "            # If depth is needed, you can extract it too\n",
    "            if \"depth\" in outputs:\n",
    "                depth = outputs[\"depth\"].cpu().numpy()\n",
    "                return rgb, depth\n",
    "\n",
    "            return rgb\n",
    "\n",
    "    def get_sensor_input(self, state):\n",
    "        \"\"\"\n",
    "        Generate sensor output (RGB image) for the given simulation state.\n",
    "        \n",
    "        Args:\n",
    "            state (dict): Current state of the simulation containing:\n",
    "                - camera_position (np.ndarray): 3D position of the camera\n",
    "                - camera_rotation (np.ndarray): Rotation of the camera (e.g., quaternion)\n",
    "                - vehicle_positions (dict): Dictionary mapping vehicle IDs to positions\n",
    "                - vehicle_rotations (dict): Dictionary mapping vehicle IDs to rotations\n",
    "                - timestamp (float): Current simulation time\n",
    "                \n",
    "        Returns:\n",
    "            dict: Sensor outputs including rendered image\n",
    "        \"\"\"\n",
    "        # Prepare frame data for rendering based on current state\n",
    "        frame_data = self.prepare_frame_data(state)\n",
    "\n",
    "        # Render the image\n",
    "        rgb_image = self.render_single_frame(frame_data)\n",
    "\n",
    "        # Create sensor output dictionary\n",
    "        sensor_output = {\n",
    "            \"rgb_image\": rgb_image,\n",
    "            # Add other sensor outputs as needed\n",
    "        }\n",
    "\n",
    "        return sensor_output\n",
    "\n",
    "    # def prepare_frame_data(self, state: State):\n",
    "    #     \"\"\"\n",
    "    #     Prepare the frame data needed for rendering based on simulation state.\n",
    "        \n",
    "    #     Args:\n",
    "    #         state (dict): Current state of the simulation\n",
    "            \n",
    "    #     Returns:\n",
    "    #         dict: Frame data dictionary with cam_infos and image_infos\n",
    "    #     \"\"\"\n",
    "    #     # Extract camera information\n",
    "    #     camera_position = torch.tensor([\n",
    "    #         state.ego_pos.x,\n",
    "    #         state.ego_pos.y,\n",
    "    #         state.ego_pos.z\n",
    "    #     ], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "    #     # Convert heading to proper quaternion in DriveStudio's coordinate system\n",
    "    #     # Create quaternion for yaw rotation (around z-axis)\n",
    "    #     # Convert heading to tensor first to avoid TypeError with numpy.float64\n",
    "    #     heading = torch.tensor(state.ego_pos.heading, dtype=torch.float32, device=self.device)\n",
    "    #     # Format quaternion as [w, x, y, z] which is the format expected by compute_camera_matrix\n",
    "    #     camera_rotation = torch.tensor([\n",
    "    #         torch.cos(heading / 2),  # w\n",
    "    #         0.0,                     # x (roll)\n",
    "    #         0.0,                     # y (pitch)\n",
    "    #         torch.sin(heading / 2)   # z (yaw)\n",
    "    #     ], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "    #     # Get camera matrix\n",
    "    #     c2w = self.compute_camera_matrix(camera_position, camera_rotation)\n",
    "        \n",
    "    #     # Correct construction of intrinsics from 9-element array\n",
    "    #     # The NuPlan format is [f_u, f_v, c_u, c_v, k1, k2, p1, p2, k3]\n",
    "    #     intrinsics_array = torch.tensor([\n",
    "    #         1.545000000000000000e+03,  # fx (f_u)\n",
    "    #         1.545000000000000000e+03,  # fy (f_v)\n",
    "    #         9.600000000000000000e+02,  # cx (c_u)\n",
    "    #         5.600000000000000000e+02,  # cy (c_v)\n",
    "    #         -3.561230000000000229e-01, # k1\n",
    "    #         1.725450000000000039e-01,  # k2\n",
    "    #         -2.129999999999999949e-03, # p1\n",
    "    #         4.640000000000000027e-04,  # p2\n",
    "    #         -5.231000000000000233e-02  # k3\n",
    "    #     ], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "    #     # Convert to proper 3x3 intrinsics matrix format\n",
    "    #     fx, fy, cx, cy = intrinsics_array[0], intrinsics_array[1], intrinsics_array[2], intrinsics_array[3]\n",
    "    #     intrinsics = torch.tensor([\n",
    "    #         [fx, 0.0, cx],\n",
    "    #         [0.0, fy, cy],\n",
    "    #         [0.0, 0.0, 1.0]\n",
    "    #     ], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "    #     # Set image dimensions that match the intrinsics\n",
    "    #     H, W = 720, 1280  # Standard dimensions for NuPlan\n",
    "        \n",
    "    #     # Create the camera information dictionary\n",
    "    #     cam_infos = {\n",
    "    #         \"camera_to_world\": c2w,\n",
    "    #         \"intrinsics\": intrinsics,\n",
    "    #         \"height\": torch.tensor(H, dtype=torch.long, device=self.device),\n",
    "    #         \"width\": torch.tensor(W, dtype=torch.long, device=self.device),\n",
    "    #         \"cam_name\": \"front_camera\",  # For logging purposes\n",
    "    #         \"cam_id\": torch.tensor(0, dtype=torch.long, device=self.device),  # Front camera ID\n",
    "    #     }\n",
    "        \n",
    "    #     # Generate ray directions and origins for all pixels\n",
    "    #     j, i = torch.meshgrid(\n",
    "    #         torch.arange(H, device=self.device),\n",
    "    #         torch.arange(W, device=self.device),\n",
    "    #         indexing='ij'\n",
    "    #     )\n",
    "        \n",
    "    #     # Convert pixel coordinates to ray directions using the intrinsics matrix\n",
    "    #     # Following the DriveStudio convention in get_rays() in pixel_source.py\n",
    "    #     # See datasets/base/pixel_source.py\n",
    "    #     directions = torch.stack([\n",
    "    #         (i - cx + 0.5) / fx,\n",
    "    #         (j - cy + 0.5) / fy,\n",
    "    #         torch.ones_like(i)\n",
    "    #     ], dim=-1)  # [H, W, 3]\n",
    "        \n",
    "    #     # Compute ray direction norms for depth scaling\n",
    "    #     direction_norm = torch.linalg.norm(directions, dim=-1, keepdim=True)\n",
    "        \n",
    "    #     # Normalize ray directions\n",
    "    #     viewdirs = directions / (direction_norm + 1e-8)\n",
    "        \n",
    "    #     # Convert from camera space to world space using c2w\n",
    "    #     # Rotating the viewdirs by the camera rotation matrix\n",
    "    #     viewdirs = (c2w[:3, :3] @ viewdirs.reshape(-1, 3).t()).t().reshape(H, W, 3)\n",
    "        \n",
    "    #     # Ray origins are the camera position for all pixels\n",
    "    #     origins = torch.broadcast_to(c2w[:3, 3], (H, W, 3))\n",
    "        \n",
    "    #     # Normalized time for the current timestamp\n",
    "    #     normalized_time = torch.full(\n",
    "    #         (H, W), \n",
    "    #         (state.timestamp.time_us - self.dataset.start_timestep) / \n",
    "    #         (self.dataset.end_timestep - self.dataset.start_timestep)\n",
    "    #     ).to(self.device)\n",
    "        \n",
    "    #     # Create frame indices and other required data for the image\n",
    "    #     image_infos = {\n",
    "    #         \"origins\": origins,\n",
    "    #         \"viewdirs\": viewdirs,\n",
    "    #         \"direction_norm\": direction_norm,\n",
    "    #         \"pixel_coords\": torch.stack([j.float() / H, i.float() / W], dim=-1),\n",
    "    #         \"normed_time\": normalized_time,\n",
    "    #         \"img_idx\": torch.full((H, W), 0, dtype=torch.long, device=self.device),\n",
    "    #         \"frame_idx\": torch.full((H, W), 0, dtype=torch.long, device=self.device),\n",
    "    #     }\n",
    "        \n",
    "    #     return {\n",
    "    #         \"cam_infos\": cam_infos,\n",
    "    #         \"image_infos\": image_infos\n",
    "    #     }\n",
    "    \n",
    "    def prepare_frame_data(self, state: State):\n",
    "        \"\"\"\n",
    "        Prepare the frame data needed for rendering based on simulation state.\n",
    "        Args:\n",
    "            state (State): Current state of the simulation\n",
    "        Returns:\n",
    "            dict: Frame data dictionary with cam_infos and image_infos\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import torch\n",
    "        from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "        # Camera position\n",
    "        camera_position = torch.tensor([\n",
    "            state.ego_pos.x,\n",
    "            state.ego_pos.y,\n",
    "            state.ego_pos.z\n",
    "        ], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Heading (yaw) to quaternion [w, x, y, z]\n",
    "        heading = float(state.ego_pos.heading)\n",
    "        quat = R.from_euler('z', heading).as_quat()  # [x, y, z, w]\n",
    "        quat = np.roll(quat, 1)  # to [w, x, y, z]\n",
    "        camera_rotation = torch.tensor(quat, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Camera-to-world matrix\n",
    "        c2w = self.compute_camera_matrix(camera_position, camera_rotation)\n",
    "\n",
    "        # Intrinsics\n",
    "        intrinsics_array = torch.tensor([\n",
    "            1.545000000000000000e+03,  # fx (f_u)\n",
    "            1.545000000000000000e+03,  # fy (f_v)\n",
    "            9.600000000000000000e+02,  # cx (c_u)\n",
    "            5.600000000000000000e+02,  # cy (c_v)\n",
    "            -3.561230000000000229e-01, # k1\n",
    "            1.725450000000000039e-01,  # k2\n",
    "            -2.129999999999999949e-03, # p1\n",
    "            4.640000000000000027e-04,  # p2\n",
    "            -5.231000000000000233e-02  # k3\n",
    "        ], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Convert to proper 3x3 intrinsics matrix format\n",
    "        fx, fy, cx, cy = intrinsics_array[0], intrinsics_array[1], intrinsics_array[2], intrinsics_array[3]\n",
    "        intrinsics = torch.tensor([\n",
    "            [fx, 0.0, cx],\n",
    "            [0.0, fy, cy],\n",
    "            [0.0, 0.0, 1.0]\n",
    "        ], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        H, W = 720, 1280\n",
    "\n",
    "        # Generate pixel grid (center of each pixel)\n",
    "        j, i = torch.meshgrid(\n",
    "            torch.arange(H, device=self.device),\n",
    "            torch.arange(W, device=self.device),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        # Pixel centers\n",
    "        pixel_i = i.float() + 0.5\n",
    "        pixel_j = j.float() + 0.5\n",
    "\n",
    "        # Directions in camera space\n",
    "        directions = torch.stack([\n",
    "            (pixel_i - cx) / fx,\n",
    "            (pixel_j - cy) / fy,\n",
    "            torch.ones_like(pixel_i)\n",
    "        ], dim=-1)  # [H, W, 3]\n",
    "\n",
    "        # Normalize directions\n",
    "        direction_norm = torch.linalg.norm(directions, dim=-1, keepdim=True)\n",
    "        viewdirs = directions / (direction_norm + 1e-8)\n",
    "\n",
    "        # Rotate directions to world space\n",
    "        viewdirs = (c2w[:3, :3] @ viewdirs.reshape(-1, 3).t()).t().reshape(H, W, 3)\n",
    "\n",
    "        # Ray origins\n",
    "        origins = torch.broadcast_to(c2w[:3, 3], (H, W, 3))\n",
    "\n",
    "        # Normalized time\n",
    "        normalized_time = torch.full(\n",
    "            (H, W),\n",
    "            (state.timestamp.time_us - self.dataset.start_timestep) /\n",
    "            (self.dataset.end_timestep - self.dataset.start_timestep),\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        # Pixel coordinates normalized to [0, 1]\n",
    "        pixel_coords = torch.stack([\n",
    "            pixel_j / H,\n",
    "            pixel_i / W\n",
    "        ], dim=-1)\n",
    "\n",
    "        cam_infos = {\n",
    "            \"camera_to_world\": c2w,\n",
    "            \"intrinsics\": intrinsics,\n",
    "            \"height\": torch.tensor(H, dtype=torch.long, device=self.device),\n",
    "            \"width\": torch.tensor(W, dtype=torch.long, device=self.device),\n",
    "            \"cam_name\": \"front_camera\",\n",
    "            \"cam_id\": torch.tensor(0, dtype=torch.long, device=self.device),\n",
    "        }\n",
    "\n",
    "        image_infos = {\n",
    "            \"origins\": origins,\n",
    "            \"viewdirs\": viewdirs,\n",
    "            \"direction_norm\": direction_norm,\n",
    "            \"pixel_coords\": pixel_coords,\n",
    "            \"normed_time\": normalized_time,\n",
    "            \"img_idx\": torch.full((H, W), 0, dtype=torch.long, device=self.device),\n",
    "            \"frame_idx\": torch.full((H, W), 0, dtype=torch.long, device=self.device),\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"cam_infos\": cam_infos,\n",
    "            \"image_infos\": image_infos\n",
    "        }\n",
    "\n",
    "    def compute_camera_matrix(self, position, rotation):\n",
    "        \"\"\"\n",
    "        Compute the camera-to-world transformation matrix from position and rotation.\n",
    "        Following DriveStudio's camera transformation convention.\n",
    "        \n",
    "        Args:\n",
    "            position (torch.Tensor): 3D position vector [x, y, z]\n",
    "            rotation (torch.Tensor): Rotation as quaternion [w, x, y, z]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: 4x4 camera-to-world transformation matrix\n",
    "        \"\"\"\n",
    "        # Create a cache key for this position and rotation\n",
    "        cache_key = (tuple(position.cpu().numpy().tolist() if isinstance(position, torch.Tensor) else position), \n",
    "                     tuple(rotation.cpu().numpy().tolist() if isinstance(rotation, torch.Tensor) else rotation))\n",
    "\n",
    "        # Check if we've already computed this matrix\n",
    "        if cache_key in self.camera_matrix_cache:\n",
    "            return self.camera_matrix_cache[cache_key]\n",
    "\n",
    "        # Initialize transformation matrix\n",
    "        c2w = torch.eye(4, device=self.device)\n",
    "\n",
    "        # Set translation component (position)\n",
    "        c2w[:3, 3] = position\n",
    "\n",
    "        # Convert quaternion to rotation matrix\n",
    "        # Note: rotation contains quaternion in order [w, x, y, z]\n",
    "        w, x, y, z = rotation\n",
    "\n",
    "        # Construct rotation matrix from quaternion\n",
    "        rot_matrix = torch.tensor([\n",
    "            [1 - 2*y*y - 2*z*z, 2*x*y - 2*w*z, 2*x*z + 2*w*y],\n",
    "            [2*x*y + 2*w*z, 1 - 2*x*x - 2*z*z, 2*y*z - 2*w*x],\n",
    "            [2*x*z - 2*w*y, 2*y*z + 2*w*x, 1 - 2*x*x - 2*y*y]\n",
    "        ], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Set rotation component of the transformation matrix\n",
    "        c2w[:3, :3] = rot_matrix\n",
    "        \n",
    "        # Apply OpenCV to dataset coordinate system transformation\n",
    "        opencv2dataset = torch.tensor(OPENCV2DATASET, dtype=torch.float32, device=self.device)\n",
    "        c2w = c2w @ opencv2dataset\n",
    "\n",
    "        # Store in cache for future use\n",
    "        self.camera_matrix_cache[cache_key] = c2w\n",
    "\n",
    "        return c2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440014d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /cluster/home/larstond/master-project/drivestudio to sys.path\n",
      "Changed working directory to /cluster/home/larstond/master-project/drivestudio\n",
      "Working directory: /cluster/home/larstond/master-project/drivestudio\n",
      "Config path (relative to drivestudio): configs/datasets/nuplan/8cams_undistorted.yaml\n",
      "Absolute config path: /cluster/home/larstond/master-project/drivestudio/configs/datasets/nuplan/8cams_undistorted.yaml\n",
      "Loading config from: configs/datasets/nuplan/8cams_undistorted.yaml\n",
      "Loading checkpoint from: output/master-project/run_omnire_undistorted_8cams_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   1%|▏         | 4/300 [00:00<00:10, 28.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:08<00:00, 35.19it/s]\n",
      "Loading dynamic masks:   3%|▎         | 9/300 [00:00<00:03, 85.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 83.48it/s]\n",
      "Loading human masks:   6%|▌         | 18/300 [00:00<00:03, 85.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:04<00:00, 64.22it/s]\n",
      "Loading vehicle masks:   3%|▎         | 8/300 [00:00<00:03, 78.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 84.71it/s]\n",
      "Loading sky masks:  10%|▉         | 29/300 [00:00<00:01, 140.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 135.19it/s]\n",
      "Loading images:   1%|▏         | 4/300 [00:00<00:08, 35.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:08<00:00, 36.97it/s]\n",
      "Loading dynamic masks:   3%|▎         | 9/300 [00:00<00:03, 85.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 88.24it/s]\n",
      "Loading human masks:   3%|▎         | 9/300 [00:00<00:03, 84.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 86.94it/s]\n",
      "Loading vehicle masks:   4%|▍         | 13/300 [00:00<00:04, 64.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 85.04it/s]\n",
      "Loading sky masks:  10%|█         | 30/300 [00:00<00:01, 145.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 144.23it/s]\n",
      "Loading images:   2%|▏         | 6/300 [00:00<00:09, 29.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:11<00:00, 26.24it/s]\n",
      "Loading dynamic masks:   2%|▏         | 6/300 [00:00<00:05, 55.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:07<00:00, 39.24it/s]\n",
      "Loading human masks:   2%|▏         | 6/300 [00:00<00:05, 54.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:06<00:00, 43.76it/s]\n",
      "Loading vehicle masks:   2%|▏         | 5/300 [00:00<00:07, 40.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:07<00:00, 40.12it/s]\n",
      "Loading sky masks:   2%|▏         | 6/300 [00:00<00:05, 51.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:04<00:00, 66.29it/s] \n",
      "Loading images:   1%|          | 3/300 [00:00<00:12, 23.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:12<00:00, 24.28it/s]\n",
      "Loading dynamic masks:   1%|▏         | 4/300 [00:00<00:09, 31.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:07<00:00, 38.02it/s]\n",
      "Loading human masks:   2%|▏         | 7/300 [00:00<00:09, 32.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:08<00:00, 37.07it/s]\n",
      "Loading vehicle masks:   1%|▏         | 4/300 [00:00<00:08, 35.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:08<00:00, 37.35it/s]\n",
      "Loading sky masks:   1%|          | 3/300 [00:00<00:11, 26.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:05<00:00, 53.09it/s]\n",
      "Loading images:   1%|          | 2/300 [00:00<00:19, 14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:  72%|███████▏  | 216/300 [00:10<00:04, 19.75it/s]"
     ]
    }
   ],
   "source": [
    "base_dir = Path.cwd()\n",
    "\n",
    "setup = None\n",
    "\n",
    "# Now use the context manager for clean path handling\n",
    "with use_path(\"drivestudio\", True):\n",
    "    # Define paths relative to the drivestudio directory\n",
    "    relative_config_path = \"configs/datasets/nuplan/8cams_undistorted.yaml\"\n",
    "    relative_checkpoint_path = \"output/master-project/run_omnire_undistorted_8cams_0\"\n",
    "    \n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "    print(f\"Config path (relative to drivestudio): {relative_config_path}\")\n",
    "    print(f\"Absolute config path: {os.path.abspath(relative_config_path)}\")\n",
    "    \n",
    "    # Check if these files exist in this context\n",
    "    if not os.path.exists(relative_config_path):\n",
    "        print(f\"ERROR: Config file not found at {os.path.abspath(relative_config_path)}\")\n",
    "    if not os.path.exists(relative_checkpoint_path):\n",
    "        print(f\"ERROR: Checkpoint directory not found at {os.path.abspath(relative_checkpoint_path)}\")\n",
    "    \n",
    "    # Only initialize if files exist\n",
    "    if os.path.exists(relative_config_path) and os.path.exists(relative_checkpoint_path):\n",
    "        setup = OmniReSetup(relative_config_path, relative_checkpoint_path)\n",
    "        print(\"Successfully initialized OmniRe environment model\")\n",
    "    else:\n",
    "        print(\"Failed to initialize environment model due to missing files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62f189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_model = OmniReModel(setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35817117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interfaces import Agent\n",
    "from nuplan.common.actor_state.state_representation import StateSE2, TimePoint, StateVector2D\n",
    "from nuplan.common.actor_state.oriented_box import OrientedBox\n",
    "from nuplan.common.actor_state.waypoint import Waypoint\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    \"\"\"Random agent that selects random actions.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_action(self, sensor_output, timestamp):\n",
    "        \"\"\"Select a random trajectory\"\"\"\n",
    "        trajectory = []\n",
    "        current_time = timestamp.time_us\n",
    "        \n",
    "        for i in range(10):\n",
    "            # Create proper TimePoint object (microseconds)\n",
    "            time_point = TimePoint(current_time + i * 100000) \n",
    "            \n",
    "            # Create position and heading\n",
    "            x = i * 10\n",
    "            y = i * 10\n",
    "            heading = -2.066\n",
    "            \n",
    "            # Create StateSE2 for position and heading\n",
    "            center = StateSE2(x, y, heading)\n",
    "            \n",
    "            # Create oriented box\n",
    "            # Parameters: center, length, width, height\n",
    "            oriented_box = OrientedBox(center, length=1.0, width=1.0, height=1.0)\n",
    "            \n",
    "            # Create velocity vector\n",
    "            velocity = StateVector2D(1.0, 0.0)  # x-velocity=1.0, y-velocity=0.0\n",
    "            \n",
    "            # Create waypoint with all required components\n",
    "            waypoint = Waypoint(time_point, oriented_box, velocity)\n",
    "            trajectory.append(waypoint)\n",
    "            \n",
    "        return trajectory\n",
    "\n",
    "agent = RandomAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2316c",
   "metadata": {},
   "source": [
    "## Do the simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f669a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "error_history = []\n",
    "sensor_outputs = []\n",
    "\n",
    "# for i in range(3):\n",
    "#     print(\"Step\", i)\n",
    "#     state = simulator.get_state()\n",
    "#     print(\"State:\", state)\n",
    "#     sensor_output = environment_model.get_sensor_input(state)\n",
    "#     sensor_outputs.append(sensor_output)\n",
    "#     print(\"Sensor Output:\", sensor_output)\n",
    "#     action = agent.get_action(sensor_output, state.timestamp)\n",
    "#     print(\"Action:\", action)\n",
    "#     simulator.do_action(action)\n",
    "#     error_history.append(simulator.get_state())\n",
    "\n",
    "start_heading = simulator.get_state().ego_pos.heading\n",
    "# Get the initial state\n",
    "print(\"Initial State:\", start_pos)\n",
    "\n",
    "for i in range(120):\n",
    "    print(\"Step\", i)\n",
    "    # Add 100 to the x of the initial state for each step\n",
    "    state = simulator.get_state()\n",
    "    # state.ego_pos.heading = start_heading + i * 0.1\n",
    "    state.ego_pos.x = start_pos.x + i * 1.0  # Move in x direction\n",
    "    state.ego_pos.y = start_pos.y + i * 0.5  # Move in y direction\n",
    "    \n",
    "    sensor_output = environment_model.get_sensor_input(state)\n",
    "    sensor_outputs.append(sensor_output)\n",
    "    \n",
    "# print(\"Error History:\", error_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70f32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rgb_image(rgb_image, gamma=2.2):\n",
    "    \"\"\"\n",
    "    Display an RGB image using matplotlib with gamma correction for better visualization.\n",
    "    \n",
    "    Args:\n",
    "        rgb_image (np.ndarray or torch.Tensor or tuple): The RGB image to display\n",
    "        gamma (float): Gamma correction value (default: 2.2 for sRGB)\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Handle different input formats first\n",
    "    if isinstance(rgb_image, tuple):\n",
    "        # If it's a tuple, take the first element (array)\n",
    "        print(\"Input is a tuple, extracting array...\")\n",
    "        rgb_image = rgb_image[0]\n",
    "    \n",
    "    # Convert torch tensor to numpy array if needed\n",
    "    if isinstance(rgb_image, torch.Tensor):\n",
    "        rgb_image = rgb_image.detach().cpu().numpy()\n",
    "    \n",
    "    # Now that we've handled the tensor case, we can safely check shape\n",
    "    print(f\"Image shape: {rgb_image.shape}, dtype: {rgb_image.dtype}\")\n",
    "    \n",
    "    # Ensure 3D array with shape (H, W, 3)\n",
    "    if rgb_image.ndim == 4:\n",
    "        # If we have a batch dimension, take the first image\n",
    "        print(\"Input has batch dimension, taking first image...\")\n",
    "        rgb_image = rgb_image[0]\n",
    "    \n",
    "    # Ensure the image is in [0, 1] range\n",
    "    if rgb_image.max() > 1.0:\n",
    "        rgb_image = rgb_image / 255.0\n",
    "    \n",
    "    # Create a figure with a specific size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Apply gamma correction for display\n",
    "    corrected_image = np.power(np.clip(rgb_image, 0, 1), 1/gamma)\n",
    "    \n",
    "    # Display the image with gamma correction\n",
    "    plt.imshow(corrected_image)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.title(f\"Gamma-corrected image (γ={gamma})\")\n",
    "    plt.show()\n",
    "\n",
    "# Try different gamma values for each image to find the optimal visualization\n",
    "for i, sensor_output in enumerate(sensor_outputs):\n",
    "    rgb_image = sensor_output[\"rgb_image\"]\n",
    "    print(f\"Image {i} with gamma=2.2 (standard sRGB):\")\n",
    "    print_rgb_image(rgb_image, gamma=2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def save_rgb_images_to_video(sensor_outputs, output_path, gamma=2.2):\n",
    "    \"\"\"\n",
    "    Save a list of RGB images to a video file with proper gamma correction.\n",
    "    \n",
    "    Args:\n",
    "        sensor_outputs (list): List of sensor outputs containing RGB images\n",
    "        output_path (str): Path to save the video file\n",
    "        gamma (float): Gamma correction value (default: 2.2 for sRGB)\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    # Get the first rgb_image and handle tuple case\n",
    "    first_rgb = sensor_outputs[0][\"rgb_image\"]\n",
    "    if isinstance(first_rgb, tuple):\n",
    "        # If it's a tuple, take the first element (RGB array)\n",
    "        first_rgb = first_rgb[0]\n",
    "    \n",
    "    # Get the dimensions of the first image\n",
    "    height, width, _ = first_rgb.shape\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    # Use mp4v codec for MP4 output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, 20.0, (width, height))\n",
    "    \n",
    "    for sensor_output in sensor_outputs:\n",
    "        rgb_image = sensor_output[\"rgb_image\"]\n",
    "        \n",
    "        # Handle tuple case\n",
    "        if isinstance(rgb_image, tuple):\n",
    "            rgb_image = rgb_image[0]  # Extract RGB array from tuple\n",
    "        \n",
    "        # Make sure values are in float [0, 1] range for consistent processing\n",
    "        if rgb_image.dtype != np.float32 and rgb_image.dtype != np.float64:\n",
    "            if rgb_image.max() > 1.0:\n",
    "                rgb_image = rgb_image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Apply gamma correction for better visual appearance\n",
    "        corrected_image = np.power(np.clip(rgb_image, 0, 1), 1/gamma)\n",
    "        \n",
    "        # Convert to 8-bit color format required for video\n",
    "        uint8_image = (corrected_image * 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert from RGB to BGR (OpenCV format)\n",
    "        bgr_image = cv2.cvtColor(uint8_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Write frame to video\n",
    "        out.write(bgr_image)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"Video saved to {output_path} with gamma correction (γ={gamma})\")\n",
    "\n",
    "# Create and save the gamma-corrected video\n",
    "output_video_path = \"output_video.mp4\"\n",
    "save_rgb_images_to_video(sensor_outputs, output_video_path, gamma=2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2197333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gamma_corrections(rgb_image, gamma_values=[1.0, 1.8, 2.2, 2.4]):\n",
    "    \"\"\"\n",
    "    Display an RGB image with different gamma correction values side by side.\n",
    "    \n",
    "    Args:\n",
    "        rgb_image (np.ndarray or torch.Tensor or tuple): The RGB image to display\n",
    "        gamma_values (list): List of gamma values to compare\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    # Handle different input formats\n",
    "    if isinstance(rgb_image, tuple):\n",
    "        rgb_image = rgb_image[0]\n",
    "    \n",
    "    if isinstance(rgb_image, torch.Tensor):\n",
    "        rgb_image = rgb_image.detach().cpu().numpy()\n",
    "    \n",
    "    if rgb_image.ndim == 4:\n",
    "        rgb_image = rgb_image[0]\n",
    "    \n",
    "    # Ensure the image is in [0, 1] range\n",
    "    if rgb_image.max() > 1.0:\n",
    "        rgb_image = rgb_image / 255.0\n",
    "    \n",
    "    # Create subplots for each gamma value\n",
    "    fig, axes = plt.subplots(1, len(gamma_values), figsize=(16, 5))\n",
    "    fig.suptitle(\"Comparison of Different Gamma Correction Values\", fontsize=16)\n",
    "    \n",
    "    for i, gamma in enumerate(gamma_values):\n",
    "        # Apply gamma correction\n",
    "        corrected_image = np.power(np.clip(rgb_image, 0, 1), 1/gamma)\n",
    "        \n",
    "        # Display in the appropriate subplot\n",
    "        axes[i].imshow(corrected_image)\n",
    "        axes[i].set_title(f\"γ = {gamma}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compare different gamma values for a sample rendered image\n",
    "if len(sensor_outputs) > 0:\n",
    "    compare_gamma_corrections(sensor_outputs[0][\"rgb_image\"])\n",
    "    \n",
    "    # You can try more advanced processing if needed\n",
    "    # For example, to enhance local contrast while maintaining global appearance:\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    \n",
    "    # Function to apply advanced image enhancements\n",
    "    def enhance_image(rgb_image, gamma=2.2, clahe_clip=2.0, clahe_grid=(8,8)):\n",
    "        \"\"\"Apply advanced image enhancement techniques\"\"\"\n",
    "        # Make sure image is in proper format\n",
    "        if isinstance(rgb_image, tuple):\n",
    "            rgb_image = rgb_image[0]\n",
    "            \n",
    "        # Convert to 8-bit for OpenCV operations\n",
    "        img_8bit = (np.clip(rgb_image, 0, 1) * 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert to LAB color space (L=lightness, A=green-red, B=blue-yellow)\n",
    "        lab = cv2.cvtColor(img_8bit, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        \n",
    "        # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to lightness channel\n",
    "        clahe = cv2.createCLAHE(clipLimit=clahe_clip, tileGridSize=clahe_grid)\n",
    "        cl = clahe.apply(l)\n",
    "        \n",
    "        # Merge channels back\n",
    "        enhanced_lab = cv2.merge((cl, a, b))\n",
    "        \n",
    "        # Convert back to RGB\n",
    "        enhanced_rgb = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        # Apply gamma correction for display\n",
    "        gamma_corrected = np.power(enhanced_rgb / 255.0, 1/gamma)\n",
    "        \n",
    "        return gamma_corrected\n",
    "    \n",
    "    # Display enhanced version of the first frame\n",
    "    if len(sensor_outputs) > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        enhanced = enhance_image(sensor_outputs[0][\"rgb_image\"])\n",
    "        plt.imshow(enhanced)\n",
    "        plt.axis('off')\n",
    "        plt.title(\"Enhanced image with CLAHE + gamma correction\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
