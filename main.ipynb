{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfff27a0",
   "metadata": {},
   "source": [
    "# Main simulator loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12c13c",
   "metadata": {},
   "source": [
    "## Imports, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d534ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from path_utils import use_path\n",
    "\n",
    "base_dir = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "\n",
    "# Use relative paths from the base directory\n",
    "config_path = str(base_dir / \"configs/datasets/nuplan/8cams_undistorted.yaml\")\n",
    "checkpoint_path = str(base_dir / \"output/master-project/run_omnire_undistorted_8cams_0\")\n",
    "n_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d48ea",
   "metadata": {},
   "source": [
    "## Initialize simulator, environment model and the random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eef672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing NuPlan simulator...\n",
      "<nuplan.common.actor_state.ego_state.EgoState object at 0x7fdcde268250>\n",
      "NuPlan initialized.\n"
     ]
    }
   ],
   "source": [
    "from simulator import Simulator, NuPlan\n",
    "simulator = NuPlan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "673af314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sim_types import State\n",
    "from environment_model import OmniReSetup\n",
    "\n",
    "class OmniReModel:\n",
    "    def __init__(self, setup: OmniReSetup):\n",
    "        self.data_cfg = setup.data_cfg\n",
    "        self.train_cfg = setup.train_cfg\n",
    "        self.trainer = setup.trainer\n",
    "        self.dataset = setup.dataset\n",
    "        self.device = setup.device\n",
    "        self.camera_matrix_cache = {}\n",
    "    \n",
    "    def render_single_frame(self, frame_data: dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Render a single frame based on provided frame data.\n",
    "        \n",
    "        Args:\n",
    "            frame_data (dict): Dictionary containing camera and image info for the frame\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: The rendered RGB image as a numpy array\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Create copies of the dictionaries to avoid modifying originals\n",
    "            cam_infos = {}\n",
    "            image_infos = {}\n",
    "            \n",
    "            # Move camera info tensors to GPU\n",
    "            for key, value in frame_data[\"cam_infos\"].items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    cam_infos[key] = value.cuda(non_blocking=True)\n",
    "                else:\n",
    "                    cam_infos[key] = value\n",
    "            \n",
    "            # Move image info tensors to GPU\n",
    "            for key, value in frame_data[\"image_infos\"].items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    image_infos[key] = value.cuda(non_blocking=True)\n",
    "                else:\n",
    "                    image_infos[key] = value\n",
    "\n",
    "            # Perform rendering\n",
    "            outputs = self.trainer(\n",
    "                image_infos=image_infos,\n",
    "                camera_infos=cam_infos,\n",
    "                novel_view=True\n",
    "            )\n",
    "\n",
    "            # Extract RGB image and return\n",
    "            rgb = outputs[\"rgb\"].cpu().numpy().clip(\n",
    "                min=1.e-6, max=1-1.e-6\n",
    "            )\n",
    "\n",
    "            # If depth is needed, you can extract it too\n",
    "            if \"depth\" in outputs:\n",
    "                depth = outputs[\"depth\"].cpu().numpy()\n",
    "                return rgb, depth\n",
    "\n",
    "            return rgb\n",
    "\n",
    "    def get_sensor_output(self, state):\n",
    "        \"\"\"\n",
    "        Generate sensor output (RGB image) for the given simulation state.\n",
    "        \n",
    "        Args:\n",
    "            state (dict): Current state of the simulation containing:\n",
    "                - camera_position (np.ndarray): 3D position of the camera\n",
    "                - camera_rotation (np.ndarray): Rotation of the camera (e.g., quaternion)\n",
    "                - vehicle_positions (dict): Dictionary mapping vehicle IDs to positions\n",
    "                - vehicle_rotations (dict): Dictionary mapping vehicle IDs to rotations\n",
    "                - timestamp (float): Current simulation time\n",
    "                \n",
    "        Returns:\n",
    "            dict: Sensor outputs including rendered image\n",
    "        \"\"\"\n",
    "        # Prepare frame data for rendering based on current state\n",
    "        frame_data = self.prepare_frame_data(state)\n",
    "\n",
    "        # Render the image\n",
    "        rgb_image = self.render_single_frame(frame_data)\n",
    "\n",
    "        # Create sensor output dictionary\n",
    "        sensor_output = {\n",
    "            \"rgb_image\": rgb_image,\n",
    "            # Add other sensor outputs as needed\n",
    "        }\n",
    "\n",
    "        return sensor_output\n",
    "\n",
    "    def prepare_frame_data(self, state: State):\n",
    "        \"\"\"\n",
    "        Prepare the frame data needed for rendering based on simulation state.\n",
    "        \n",
    "        Args:\n",
    "            state (dict): Current state of the simulation\n",
    "            \n",
    "        Returns:\n",
    "            dict: Frame data dictionary with cam_infos and image_infos\n",
    "        \"\"\"\n",
    "        # Extract camera information\n",
    "        camera_position = torch.tensor([\n",
    "            state.ego_pos.x,\n",
    "            state.ego_pos.y,\n",
    "            state.ego_pos.z\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        # Assuming state.ego_pos.heading is a scalar representing the yaw angle\n",
    "        heading = torch.tensor(state.ego_pos.heading)\n",
    "        half_yaw = heading / 2\n",
    "        camera_rotation = torch.tensor([\n",
    "            0.0,                     # x (roll)\n",
    "            0.0,                     # y (pitch)\n",
    "            torch.sin(half_yaw),    # z\n",
    "            torch.cos(half_yaw)     # w\n",
    "        ])\n",
    "        \n",
    "        timestamp = state.timestamp.time_us\n",
    "\n",
    "        # Get camera matrix\n",
    "        c2w = self.compute_camera_matrix(camera_position, camera_rotation)\n",
    "        \n",
    "        # these are the intrinsics for the camera TODO: load from file\n",
    "        # 1.545000000000000000e+03\n",
    "        # 1.545000000000000000e+03\n",
    "        # 9.600000000000000000e+02\n",
    "        # 5.600000000000000000e+02\n",
    "        # -3.561230000000000229e-01\n",
    "        # 1.725450000000000039e-01\n",
    "        # -2.129999999999999949e-03\n",
    "        # 4.640000000000000027e-04\n",
    "        # -5.231000000000000233e-02\n",
    "        \n",
    "        intrinsics = torch.tensor([\n",
    "            1.545000000000000000e+03,  # fx\n",
    "            1.545000000000000000e+03,  # fy\n",
    "            9.600000000000000000e+02,  # cx\n",
    "            5.600000000000000000e+02,  # cy\n",
    "            -3.561230000000000229e-01, # skew_x\n",
    "            1.725450000000000039e-01,  # skew_y\n",
    "            -2.129999999999999949e-03, # skew_xy\n",
    "            4.640000000000000027e-04,  # skew_yx\n",
    "            -5.231000000000000233e-02   # skew_yy\n",
    "        ], dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Reshape intrinsics to match the expected input shape [3,3]\n",
    "        intrinsics = intrinsics.view(3, 3).to(self.device)\n",
    "\n",
    "        # Create camera information dictionary\n",
    "        cam_infos = {\n",
    "            \"intrinsics\": intrinsics,  # Placeholder for intrinsics\n",
    "            \"width\": torch.tensor([1280], device=self.device),\n",
    "            \"height\": torch.tensor([720], device=self.device),\n",
    "            \"camera_to_world\": c2w,\n",
    "            # Add other camera parameters required by your model\n",
    "        }\n",
    "\n",
    "        # Vehicle information\n",
    "        vehicles = getattr(state, \"vehicle_pos_list\", {})\n",
    "        \n",
    "        # vehicles is on the form Position(x=664432.4584765462, y=3998282.198022293, z=0, heading=-1.5377766955975682\n",
    "        \n",
    "        vehicle_positions = {\n",
    "            vehicle_id: np.array([vehicle.x, vehicle.y, vehicle.z], dtype=np.float32)\n",
    "            for vehicle_id, vehicle in enumerate(vehicles)\n",
    "        }\n",
    "        vehicle_rotations = {\n",
    "            vehicle_id: np.array([0.0, 0.0, np.sin(vehicle.heading / 2), np.cos(vehicle.heading / 2)], dtype=np.float32)\n",
    "            for vehicle_id, vehicle in enumerate(vehicles)\n",
    "        }\n",
    "\n",
    "        # Create vehicle information for the renderer\n",
    "        # This depends on how your renderer expects vehicle data\n",
    "        # vehicle_data = {\n",
    "        #     vehicle_id: {\n",
    "        #         \"position\": torch.tensor(position, device=self.device),\n",
    "        #         \"rotation\": torch.tensor(vehicle_rotations.get(vehicle_id, np.array([1.0, 0.0, 0.0, 0.0])), device=self.device),\n",
    "        #     }\n",
    "        #     for vehicle_id, position in vehicle_positions.items()\n",
    "        # }\n",
    "        \n",
    "        vehicle_data = {\n",
    "            vehicle_id: {\n",
    "                \"position\": torch.tensor(position, device=self.device),\n",
    "                \"rotation\": torch.tensor(vehicle_rotations.get(vehicle_id, np.array([1.0, 0.0, 0.0, 0.0])), device=self.device),\n",
    "            }\n",
    "            for vehicle_id, position in vehicle_positions.items()\n",
    "        }\n",
    "\n",
    "        # Create image information dictionary\n",
    "        # This includes any additional data needed for rendering\n",
    "        \n",
    "        normalized_time = (timestamp - self.dataset.start_timestep) / (self.dataset.end_timestep - self.dataset.start_timestep)\n",
    "        \n",
    "        image_infos = {\n",
    "            \"timestamp\": torch.tensor([timestamp], device=self.device),\n",
    "            \"vehicles\": vehicle_data,\n",
    "            \"normed_time\": torch.tensor([normalized_time], device=self.device),\n",
    "            \"img_idx\": torch.tensor([0]),\n",
    "            \"viewdirs\": self.compute_viewdirs(\n",
    "                intrinsics=cam_infos[\"intrinsics\"],\n",
    "                H=cam_infos[\"height\"],\n",
    "                W=cam_infos[\"width\"],\n",
    "                device=self.device\n",
    "            )\n",
    "            # Add other required image information\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"cam_infos\": cam_infos,\n",
    "            \"image_infos\": image_infos\n",
    "        }\n",
    "    \n",
    "    def compute_viewdirs(self, intrinsics, H, W, device):\n",
    "        \"\"\"Returns (H, W, 3) ray directions in camera coordinates\"\"\"\n",
    "        fx, fy = intrinsics[0, 0], intrinsics[1, 1]\n",
    "        cx, cy = intrinsics[0, 2], intrinsics[1, 2]\n",
    "\n",
    "        i, j = torch.meshgrid(\n",
    "            torch.arange(W, device=device),\n",
    "            torch.arange(H, device=device),\n",
    "            indexing='xy'\n",
    "        )\n",
    "        dirs = torch.stack([\n",
    "            (i - cx) / fx,\n",
    "            (j - cy) / fy,\n",
    "            torch.ones_like(i)\n",
    "        ], dim=-1)  # [H, W, 3]\n",
    "        dirs = dirs / dirs.norm(dim=-1, keepdim=True)  # Normalize\n",
    "        return dirs\n",
    "\n",
    "    def compute_camera_matrix(self, position, rotation):\n",
    "        \"\"\"\n",
    "        Compute the camera-to-world transformation matrix from position and rotation.\n",
    "        \n",
    "        Args:\n",
    "            position (np.ndarray): 3D position vector [x, y, z]\n",
    "            rotation (np.ndarray): Rotation as quaternion [w, x, y, z]\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: 4x4 camera-to-world transformation matrix\n",
    "        \"\"\"\n",
    "        # Create a cache key for this position and rotation\n",
    "        cache_key = (tuple(position), tuple(rotation))\n",
    "\n",
    "        # Check if we've already computed this matrix\n",
    "        if cache_key in self.camera_matrix_cache:\n",
    "            return self.camera_matrix_cache[cache_key]\n",
    "\n",
    "        # Convert position to tensor\n",
    "        position_tensor = torch.tensor(position, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Initialize transformation matrix\n",
    "        c2w = torch.eye(4, device=self.device)\n",
    "\n",
    "        # Set translation component\n",
    "        c2w[:3, 3] = position_tensor\n",
    "\n",
    "        # Convert quaternion to rotation matrix\n",
    "        # Assuming quaternion is [w, x, y, z]\n",
    "        w, x, y, z = rotation\n",
    "\n",
    "        # Construct rotation matrix from quaternion\n",
    "        rot_matrix = torch.tensor([\n",
    "            [1 - 2*y*y - 2*z*z, 2*x*y - 2*w*z, 2*x*z + 2*w*y],\n",
    "            [2*x*y + 2*w*z, 1 - 2*x*x - 2*z*z, 2*y*z - 2*w*x],\n",
    "            [2*x*z - 2*w*y, 2*y*z + 2*w*x, 1 - 2*x*x - 2*y*y]\n",
    "        ], dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # Set rotation component of the transformation matrix\n",
    "        c2w[:3, :3] = rot_matrix\n",
    "\n",
    "        # Store in cache for future use\n",
    "        self.camera_matrix_cache[cache_key] = c2w\n",
    "\n",
    "        return c2w\n",
    "\n",
    "    def update_vehicle_positions(self, state, vehicle_id, new_position, new_rotation):\n",
    "        \"\"\"\n",
    "        Update the position and rotation of a specific vehicle in the state.\n",
    "        \n",
    "        Args:\n",
    "            state (dict): Current simulation state\n",
    "            vehicle_id (int): ID of the vehicle to update\n",
    "            new_position (np.ndarray): New 3D position\n",
    "            new_rotation (np.ndarray): New rotation (quaternion)\n",
    "            \n",
    "        Returns:\n",
    "            dict: Updated state dictionary\n",
    "        \"\"\"\n",
    "        # Create a copy of the state to avoid modifying the original\n",
    "        updated_state = state.copy()\n",
    "\n",
    "        # Initialize vehicle dictionaries if they don't exist\n",
    "        if \"vehicle_positions\" not in updated_state:\n",
    "            updated_state[\"vehicle_positions\"] = {}\n",
    "        if \"vehicle_rotations\" not in updated_state:\n",
    "            updated_state[\"vehicle_rotations\"] = {}\n",
    "\n",
    "        # Update vehicle position and rotation\n",
    "        updated_state[\"vehicle_positions\"][vehicle_id] = new_position\n",
    "        updated_state[\"vehicle_rotations\"][vehicle_id] = new_rotation\n",
    "\n",
    "        return updated_state\n",
    "\n",
    "    def create_trajectory(self, start_position, end_position, num_steps):\n",
    "        \"\"\"\n",
    "        Create a linear trajectory between two positions.\n",
    "        \n",
    "        Args:\n",
    "            start_position (np.ndarray): Starting position [x, y, z]\n",
    "            end_position (np.ndarray): Ending position [x, y, z]\n",
    "            num_steps (int): Number of steps in the trajectory\n",
    "            \n",
    "        Returns:\n",
    "            np.ndarray: Array of positions along the trajectory\n",
    "        \"\"\"\n",
    "        return np.linspace(start_position, end_position, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "440014d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /cluster/home/larstond/master-project/drivestudio to sys.path\n",
      "Changed working directory to /cluster/home/larstond/master-project/drivestudio\n",
      "Working directory: /cluster/home/larstond/master-project/drivestudio\n",
      "Config path (relative to drivestudio): configs/datasets/nuplan/8cams_undistorted.yaml\n",
      "Absolute config path: /cluster/home/larstond/master-project/drivestudio/configs/datasets/nuplan/8cams_undistorted.yaml\n",
      "Loading config from: configs/datasets/nuplan/8cams_undistorted.yaml\n",
      "Loading checkpoint from: output/master-project/run_omnire_undistorted_8cams_0\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images:   1%|▏         | 4/300 [00:00<00:15, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:07<00:00, 38.62it/s]\n",
      "Loading dynamic masks:   2%|▏         | 5/300 [00:00<00:06, 45.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 79.65it/s]\n",
      "Loading human masks:   3%|▎         | 9/300 [00:00<00:03, 82.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 87.57it/s]\n",
      "Loading vehicle masks:   3%|▎         | 9/300 [00:00<00:03, 86.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 83.66it/s]\n",
      "Loading sky masks:   4%|▍         | 13/300 [00:00<00:02, 119.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 123.88it/s]\n",
      "Loading images:   2%|▏         | 6/300 [00:00<00:05, 54.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:06<00:00, 49.99it/s]\n",
      "Loading dynamic masks:   3%|▎         | 8/300 [00:00<00:03, 73.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 85.37it/s]\n",
      "Loading human masks:   6%|▌         | 18/300 [00:00<00:03, 88.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 87.10it/s]\n",
      "Loading vehicle masks:   6%|▌         | 18/300 [00:00<00:03, 87.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 85.52it/s]\n",
      "Loading sky masks:   5%|▍         | 14/300 [00:00<00:02, 132.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 125.38it/s]\n",
      "Loading images:   3%|▎         | 9/300 [00:00<00:06, 45.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:06<00:00, 49.96it/s]\n",
      "Loading dynamic masks:   2%|▏         | 5/300 [00:00<00:05, 49.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 86.13it/s]\n",
      "Loading human masks:   6%|▌         | 18/300 [00:00<00:03, 86.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 87.50it/s]\n",
      "Loading vehicle masks:   6%|▌         | 18/300 [00:00<00:03, 88.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 87.47it/s]\n",
      "Loading sky masks:   9%|▊         | 26/300 [00:00<00:02, 127.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 123.74it/s]\n",
      "Loading images:   3%|▎         | 10/300 [00:00<00:06, 47.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:06<00:00, 46.28it/s]\n",
      "Loading dynamic masks:   6%|▌         | 17/300 [00:00<00:03, 81.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 85.42it/s]\n",
      "Loading human masks:   5%|▌         | 15/300 [00:00<00:03, 72.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 86.65it/s]\n",
      "Loading vehicle masks:   6%|▌         | 18/300 [00:00<00:03, 87.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 86.82it/s]\n",
      "Loading sky masks:   7%|▋         | 22/300 [00:00<00:02, 110.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 126.24it/s]\n",
      "Loading images:   3%|▎         | 10/300 [00:00<00:06, 47.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:06<00:00, 46.27it/s]\n",
      "Loading dynamic masks:   3%|▎         | 10/300 [00:00<00:03, 91.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 87.58it/s]\n",
      "Loading human masks:   6%|▌         | 18/300 [00:00<00:03, 87.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 88.08it/s]\n",
      "Loading vehicle masks:   3%|▎         | 9/300 [00:00<00:03, 84.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 86.20it/s]\n",
      "Loading sky masks:   3%|▎         | 10/300 [00:00<00:02, 96.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 124.15it/s]\n",
      "Loading images:   2%|▏         | 6/300 [00:00<00:09, 30.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:06<00:00, 47.92it/s]\n",
      "Loading dynamic masks:   2%|▏         | 6/300 [00:00<00:05, 54.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 82.36it/s]\n",
      "Loading human masks:   6%|▌         | 18/300 [00:00<00:03, 86.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 87.17it/s]\n",
      "Loading vehicle masks:   3%|▎         | 9/300 [00:00<00:03, 89.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 85.63it/s]\n",
      "Loading sky masks:   4%|▍         | 13/300 [00:00<00:02, 126.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 123.62it/s]\n",
      "Loading images:   2%|▏         | 6/300 [00:00<00:05, 51.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:07<00:00, 42.07it/s]\n",
      "Loading dynamic masks:   3%|▎         | 9/300 [00:00<00:03, 86.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 87.56it/s]\n",
      "Loading human masks:   3%|▎         | 9/300 [00:00<00:03, 83.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 86.78it/s]\n",
      "Loading vehicle masks:   3%|▎         | 9/300 [00:00<00:03, 86.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 88.07it/s]\n",
      "Loading sky masks:   9%|▊         | 26/300 [00:00<00:02, 127.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 125.29it/s]\n",
      "Loading images:   2%|▏         | 6/300 [00:00<00:05, 52.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting rgb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images: 100%|██████████| 300/300 [00:06<00:00, 49.62it/s]\n",
      "Loading dynamic masks:   2%|▏         | 5/300 [00:00<00:06, 47.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting dynamic mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dynamic masks: 100%|██████████| 300/300 [00:03<00:00, 78.72it/s]\n",
      "Loading human masks:   6%|▌         | 18/300 [00:00<00:03, 87.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting human mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading human masks: 100%|██████████| 300/300 [00:03<00:00, 87.48it/s]\n",
      "Loading vehicle masks:   6%|▌         | 18/300 [00:00<00:03, 86.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting vehicle mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading vehicle masks: 100%|██████████| 300/300 [00:03<00:00, 85.15it/s]\n",
      "Loading sky masks:   3%|▎         | 8/300 [00:00<00:03, 75.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undistorting sky mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading sky masks: 100%|██████████| 300/300 [00:02<00:00, 107.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End timestep: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading SMPL: 100%|██████████| 300/300 [00:01<00:00, 290.71it/s]\n",
      "Loading lidar:   0%|          | 0/300 [00:00<?, ?it/s]/cluster/home/larstond/master-project/drivestudio/datasets/nuplan/nuplan_sourceloader.py:417: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  lidar_points = torch.from_numpy(lidar_info[:, :3]).float()\n",
      "Loading lidar: 100%|██████████| 300/300 [00:27<00:00, 10.96it/s]\n",
      "Projecting lidar pts on images for camera CAM_F0: 100%|██████████| 300/300 [01:28<00:00,  3.38it/s]\n",
      "Projecting lidar pts on images for camera CAM_L0: 100%|██████████| 300/300 [01:35<00:00,  3.14it/s]\n",
      "Projecting lidar pts on images for camera CAM_R0: 100%|██████████| 300/300 [01:47<00:00,  2.80it/s]\n",
      "Projecting lidar pts on images for camera CAM_L1: 100%|██████████| 300/300 [02:00<00:00,  2.49it/s]\n",
      "Projecting lidar pts on images for camera CAM_R1: 100%|██████████| 300/300 [02:13<00:00,  2.25it/s]\n",
      "Projecting lidar pts on images for camera CAM_L2: 100%|██████████| 300/300 [02:11<00:00,  2.27it/s]\n",
      "Projecting lidar pts on images for camera CAM_R2: 100%|██████████| 300/300 [01:50<00:00,  2.71it/s]\n",
      "Projecting lidar pts on images for camera CAM_B0: 100%|██████████| 300/300 [01:33<00:00,  3.20it/s]\n",
      "/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/cluster/home/larstond/.conda/envs/master/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using predefined pose: da_pose\n",
      "OmniRe environment model initialized with checkpoint: output/master-project/run_omnire_undistorted_8cams_0/config.yaml\n",
      "Successfully initialized OmniRe environment model\n",
      "Restored original sys.path\n",
      "Restored working directory to /cluster/home/larstond/master-project\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path.cwd()\n",
    "\n",
    "setup = None\n",
    "\n",
    "# Now use the context manager for clean path handling\n",
    "with use_path(\"drivestudio\", True):\n",
    "    # Define paths relative to the drivestudio directory\n",
    "    relative_config_path = \"configs/datasets/nuplan/8cams_undistorted.yaml\"\n",
    "    relative_checkpoint_path = \"output/master-project/run_omnire_undistorted_8cams_0\"\n",
    "    \n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "    print(f\"Config path (relative to drivestudio): {relative_config_path}\")\n",
    "    print(f\"Absolute config path: {os.path.abspath(relative_config_path)}\")\n",
    "    \n",
    "    # Check if these files exist in this context\n",
    "    if not os.path.exists(relative_config_path):\n",
    "        print(f\"ERROR: Config file not found at {os.path.abspath(relative_config_path)}\")\n",
    "    if not os.path.exists(relative_checkpoint_path):\n",
    "        print(f\"ERROR: Checkpoint directory not found at {os.path.abspath(relative_checkpoint_path)}\")\n",
    "    \n",
    "    # Only initialize if files exist\n",
    "    if os.path.exists(relative_config_path) and os.path.exists(relative_checkpoint_path):\n",
    "        setup = OmniReSetup(relative_config_path, relative_checkpoint_path)\n",
    "        print(\"Successfully initialized OmniRe environment model\")\n",
    "    else:\n",
    "        print(\"Failed to initialize environment model due to missing files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a62f189b",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_model = OmniReModel(setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35817117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent, RandomAgent\n",
    "agent = RandomAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2316c",
   "metadata": {},
   "source": [
    "## Do the simulation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19f669a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_942891/32387.py:253: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  position_tensor = torch.tensor(position, dtype=torch.float32, device=self.device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "arange() received an invalid combination of arguments - got (Tensor, device=torch.device), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[1;32m      4\u001b[0m     state \u001b[38;5;241m=\u001b[39m simulator\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m----> 5\u001b[0m     sensor_output \u001b[38;5;241m=\u001b[39m \u001b[43menvironment_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sensor_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(sensor_output)\n\u001b[1;32m      7\u001b[0m     simulator\u001b[38;5;241m.\u001b[39mdo_action(action)\n",
      "Cell \u001b[0;32mIn[36], line 80\u001b[0m, in \u001b[0;36mOmniReModel.get_sensor_output\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03mGenerate sensor output (RGB image) for the given simulation state.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    dict: Sensor outputs including rendered image\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Prepare frame data for rendering based on current state\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m frame_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_frame_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Render the image\u001b[39;00m\n\u001b[1;32m     83\u001b[0m rgb_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_single_frame(frame_data)\n",
      "Cell \u001b[0;32mIn[36], line 202\u001b[0m, in \u001b[0;36mOmniReModel.prepare_frame_data\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;66;03m# Create image information dictionary\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# This includes any additional data needed for rendering\u001b[39;00m\n\u001b[1;32m    195\u001b[0m normalized_time \u001b[38;5;241m=\u001b[39m (timestamp \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mstart_timestep) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mend_timestep \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mstart_timestep)\n\u001b[1;32m    197\u001b[0m image_infos \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([timestamp], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvehicles\u001b[39m\u001b[38;5;124m\"\u001b[39m: vehicle_data,\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormed_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([normalized_time], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviewdirs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_viewdirs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintrinsics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcam_infos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mintrinsics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcam_infos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcam_infos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Add other required image information\u001b[39;00m\n\u001b[1;32m    209\u001b[0m }\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcam_infos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cam_infos,\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_infos\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_infos\n\u001b[1;32m    214\u001b[0m }\n",
      "Cell \u001b[0;32mIn[36], line 222\u001b[0m, in \u001b[0;36mOmniReModel.compute_viewdirs\u001b[0;34m(self, intrinsics, H, W, device)\u001b[0m\n\u001b[1;32m    218\u001b[0m fx, fy \u001b[38;5;241m=\u001b[39m intrinsics[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], intrinsics[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    219\u001b[0m cx, cy \u001b[38;5;241m=\u001b[39m intrinsics[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m], intrinsics[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    221\u001b[0m i, j \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmeshgrid(\n\u001b[0;32m--> 222\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    223\u001b[0m     torch\u001b[38;5;241m.\u001b[39marange(H, device\u001b[38;5;241m=\u001b[39mdevice),\n\u001b[1;32m    224\u001b[0m     indexing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    225\u001b[0m )\n\u001b[1;32m    226\u001b[0m dirs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m    227\u001b[0m     (i \u001b[38;5;241m-\u001b[39m cx) \u001b[38;5;241m/\u001b[39m fx,\n\u001b[1;32m    228\u001b[0m     (j \u001b[38;5;241m-\u001b[39m cy) \u001b[38;5;241m/\u001b[39m fy,\n\u001b[1;32m    229\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones_like(i)\n\u001b[1;32m    230\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [H, W, 3]\u001b[39;00m\n\u001b[1;32m    231\u001b[0m dirs \u001b[38;5;241m=\u001b[39m dirs \u001b[38;5;241m/\u001b[39m dirs\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: arange() received an invalid combination of arguments - got (Tensor, device=torch.device), but expected one of:\n * (Number end, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, *, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (Number start, Number end, Number step, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "error_history = []\n",
    "\n",
    "for _ in range(n_steps):\n",
    "    state = simulator.get_state()\n",
    "    sensor_output = environment_model.get_sensor_output(state)\n",
    "    action = agent.get_action(sensor_output)\n",
    "    simulator.do_action(action)\n",
    "    error_history.append(simulator.get_state())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
